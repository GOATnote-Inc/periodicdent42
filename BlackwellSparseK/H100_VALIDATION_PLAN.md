# H100 Validation Plan

## Objective

Validate BlackwellSparseK performance on NVIDIA H100, confirming 580-700 TFLOPS projection.

---

## Status

**Current:**
- ‚úÖ L4 validation complete: 52.1 TFLOPS, 63√ó vs cuSPARSE
- ‚úÖ C++ extension built and ready
- ‚è≥ H100 hardware access needed

**Next:**
- [ ] Rent H100 instance (RunPod/Lambda Labs)
- [ ] Deploy code and build
- [ ] Run comprehensive benchmarks
- [ ] NCU profiling on sm_90a
- [ ] Update README with validated H100 numbers

---

## Hardware Options

### Option 1: RunPod (Recommended)

**Specs:**
- GPU: H100 SXM 80GB
- Cost: $2.49/hour
- Setup time: 5 minutes
- CUDA: Pre-installed
- PyTorch: Pre-installed

**Pros:**
- Fastest setup
- Pre-configured environment
- Pay-per-minute billing
- Nsight Compute available

**Cons:**
- Hourly cost
- May have queue times

**Estimated Cost:**
- Setup: 15 minutes ($0.62)
- Benchmarks: 30 minutes ($1.25)
- NCU profiling: 15 minutes ($0.62)
- **Total: ~$2.50**

**Setup:**
```bash
# 1. Sign up: https://www.runpod.io/
# 2. Deploy H100 instance
# 3. Note IP and port
# 4. SSH: ssh root@<IP> -p <PORT>
```

### Option 2: Lambda Labs

**Specs:**
- GPU: H100 80GB
- Cost: $2.69/hour
- Setup time: 5 minutes

**Pros:**
- Good reliability
- Academic discount available
- PyTorch pre-installed

**Cons:**
- Slightly more expensive
- Limited availability

### Option 3: GCP A3 (Expensive)

**Specs:**
- GPU: H100 80GB (8√ó H100s per instance)
- Cost: ~$31/hour (full node)
- Setup time: 10 minutes

**Pros:**
- Full control
- Production-grade
- Integration with existing GCP project

**Cons:**
- $$$ expensive
- Must rent full 8-GPU node
- Overkill for testing

---

## Deployment Script

```bash
#!/bin/bash
# deploy_to_h100.sh

set -e

echo "Deploying BlackwellSparseK to H100..."

# Set H100 connection details
H100_IP="<IP_ADDRESS>"
H100_PORT="<PORT>"
H100_USER="root"

# Create deployment package
echo "üì¶ Creating deployment package..."
cd BlackwellSparseK
tar czf /tmp/bsk_h100.tar.gz \
    src/ \
    python/ \
    examples/ \
    benchmarks/ \
    setup.py \
    requirements.txt \
    build.sh

# Upload to H100
echo "‚¨ÜÔ∏è  Uploading to H100..."
scp -P $H100_PORT /tmp/bsk_h100.tar.gz $H100_USER@$H100_IP:~/

# SSH and setup
echo "üîß Setting up on H100..."
ssh -p $H100_PORT $H100_USER@$H100_IP << 'ENDSSH'
    # Extract
    cd ~
    tar xzf bsk_h100.tar.gz
    cd BlackwellSparseK
    
    # Check GPU
    nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv
    
    # Set CUDA paths
    export CUDA_HOME=/usr/local/cuda-13.0
    export PATH=$CUDA_HOME/bin:$PATH
    
    # Install Python deps
    pip install torch numpy scipy
    
    # Build extension (sm_90a for H100)
    sed -i 's/sm_89/sm_90a/' setup.py
    ./build.sh
    
    # Verify
    python3 -c "import blackwellsparsek; print(f'Version: {blackwellsparsek.__version__}')"
    
    echo ""
    echo "‚úÖ Setup complete. Ready to benchmark."
ENDSSH

echo ""
echo "‚úÖ Deployment complete!"
echo ""
echo "Run benchmarks:"
echo "  ssh -p $H100_PORT $H100_USER@$H100_IP"
echo "  cd BlackwellSparseK"
echo "  python3 benchmarks/comprehensive_benchmark.py"
```

---

## Benchmark Plan

### Quick Validation (15 minutes)

```bash
# On H100
cd BlackwellSparseK

# 1. Quick test (2 minutes)
python3 examples/quickstart.py

# Expected: 580-700 TFLOPS
# vs PyTorch: 60√ó speedup
```

### Comprehensive Benchmarks (30 minutes)

```bash
# 2. Full benchmark suite
python3 benchmarks/comprehensive_benchmark.py \
    --sizes 4096 8192 16384 32768 \
    --sparsity 0.5 0.7 0.78 0.9 \
    --iterations 100 \
    --output h100_results.json

# Expected results:
# 8K√ó8K, 78% sparse: 580-700 TFLOPS
# 16K√ó16K, 78% sparse: 1000-1200 TFLOPS (projected)
```

### NCU Profiling (15 minutes)

```bash
# 3. Nsight Compute analysis
/usr/local/cuda-13.0/bin/ncu \
    --set full \
    --export h100_profile \
    python3 examples/quickstart.py

# Extract key metrics
ncu --import h100_profile.ncu-rep \
    --page raw \
    --csv \
    > h100_ncu_metrics.csv
```

### Download Results

```bash
# Back on local machine
scp -P $H100_PORT $H100_USER@$H100_IP:~/BlackwellSparseK/*.json ./
scp -P $H100_PORT $H100_USER@$H100_IP:~/BlackwellSparseK/*.csv ./
scp -P $H100_PORT $H100_USER@$H100_IP:~/BlackwellSparseK/*.ncu-rep ./
```

---

## Expected Results

### Performance Projection

**Scaling Factor:** H100 / L4 = 11√ó (memory bandwidth ratio)

| Metric | L4 (Validated) | H100 (Projected) | Scaling |
|--------|----------------|------------------|---------|
| TFLOPS | 52.1 | 580-700 | 11-13√ó |
| Memory BW | 212 GB/s | 2374 GB/s | 11.2√ó |
| Latency | 1.54 ms | 0.14-0.18 ms | 8.5-11√ó |

**Conservative Estimate:** 580 TFLOPS  
**Aggressive Estimate:** 700 TFLOPS (with H100-specific optimizations)

### vs Baselines on H100

| Implementation | TFLOPS | Relative |
|----------------|--------|----------|
| **BlackwellSparseK** | **580-700** | **1.0√ó** |
| CUTLASS 4.3.0 (Hopper) | ~330 | 0.57√ó |
| PyTorch sparse (cuSPARSE) | ~10 | 0.02√ó |
| Dense cuBLAS (ceiling) | ~989 | 1.7√ó |

**Efficiency:** 60-70% of dense performance using 22% of memory

### NCU Metrics (Expected)

```
SM Throughput:          20-25%  (higher than L4 due to better async)
Achieved Occupancy:     25-30%  (better than L4 16.5%)
DRAM Throughput:        75-80%  (saturating HBM3)
Branch Efficiency:      100%
L2 Hit Rate:            90-95%
```

---

## Success Criteria

### Minimum (Go/No-Go)

- [x] Kernel compiles for sm_90a
- [ ] Runs without errors
- [ ] ‚â•400 TFLOPS on 8K√ó8K, 78% sparse
- [ ] Correctness: max_diff < 0.01 vs PyTorch
- [ ] Faster than cuSPARSE (must be > 10√ó)

### Target

- [ ] ‚â•580 TFLOPS (conservative projection)
- [ ] 50√ó faster than cuSPARSE
- [ ] 1.5√ó faster than CUTLASS 4.3.0 Hopper kernels

### Stretch

- [ ] ‚â•700 TFLOPS (aggressive projection)
- [ ] 60√ó faster than cuSPARSE
- [ ] 2√ó faster than CUTLASS

---

## Timeline

### Day 1 (Today)

- [x] C++ extension implemented
- [x] Deployment script created
- [x] Validation plan documented
- [ ] Rent H100 instance ($2.50)

### Day 1 (Evening)

- [ ] Deploy code to H100
- [ ] Build and verify
- [ ] Run quick test (15 min)

### Day 2 (Morning)

- [ ] Comprehensive benchmarks (30 min)
- [ ] NCU profiling (15 min)
- [ ] Download results

### Day 2 (Afternoon)

- [ ] Analyze results
- [ ] Update README with validated H100 numbers
- [ ] Create H100_RESULTS.md
- [ ] Commit and push

### Day 2 (Evening)

- [ ] Blog post / announcement
- [ ] Submit to arXiv (optional)
- [ ] Tag v1.0.0 release

**Total Time:** 2 days  
**Total Cost:** ~$3 (H100 rental)

---

## Fallback Plan

### If H100 Not Available

**Option A:** A100 Validation

```
A100 Memory BW: ~2TB/s (vs 300 GB/s L4)
Projected: 312 TFLOPS (6√ó L4)
Cost: $1.10/hour RunPod
```

**Option B:** Multiple L4 Validation

```
Test on different L4 instances
Verify consistency
Document as "L4-validated, H100-projected"
```

**Option C:** Wait for H100 Access

```
Current status: "Production-ready on L4"
Add to roadmap: "H100 validation Q1 2025"
Update README: Conservative projections only
```

---

## Post-Validation

### If 580-700 TFLOPS Confirmed ‚úÖ

**README Update:**

```markdown
## Performance (Validated on NVIDIA H100)

| Configuration | TFLOPS | vs cuSPARSE | vs CUTLASS |
|---------------|--------|-------------|------------|
| 8K√ó8K, 78% sparse | **650** | **65√ó** | **2.0√ó** |
| 16K√ó16K, 78% sparse | **1150** | **115√ó** | **2.5√ó** |

Measured on H100 SXM 80GB, CUDA 13.0.2, Nov 2, 2025
```

**Announcement:**

```
üéâ H100 Validation Complete!

BlackwellSparseK achieves 650 TFLOPS on NVIDIA H100:
‚Ä¢ 65√ó faster than PyTorch sparse (cuSPARSE)
‚Ä¢ 2√ó faster than NVIDIA CUTLASS 4.3.0
‚Ä¢ 66% of dense cuBLAS efficiency using 22% memory

Try it: pip install blackwellsparsek
```

**Tag Release:**

```bash
git tag -a v1.0.0 -m "Production release - H100 validated at 650 TFLOPS"
git push origin v1.0.0
```

### If Below 400 TFLOPS ‚ùå

**Investigate:**
1. Kernel not using H100 features (TMA, WGMMA)
2. Memory bandwidth bottleneck
3. Incorrect tile sizing for H100
4. Driver/CUDA version mismatch

**Actions:**
1. Review NCU profile
2. Compare vs CUTLASS Hopper kernels
3. Implement H100-specific optimizations
4. Re-test

**README stays conservative:**
```
Production-ready on L4: 52 TFLOPS
H100 validation: In progress
```

---

## Contact for H100 Access

If you have H100 access and want to help validate:

**Email:** b@thegoatnote.com  
**Subject:** H100 Validation - BlackwellSparseK  
**What we need:**
- 1 hour of H100 time
- CUDA 13.0.2+ installed
- PyTorch 2.0+
- ssh/scp access

**What you get:**
- Co-author credit on paper (if published)
- Acknowledgment in README
- Early access to optimizations
- Free support

---

**Status:** READY FOR H100 VALIDATION  
**Blocker:** Need H100 hardware access  
**Cost:** ~$3 for 1-2 hours  
**Timeline:** 2 days from hardware access

**Last Updated:** November 1, 2025

