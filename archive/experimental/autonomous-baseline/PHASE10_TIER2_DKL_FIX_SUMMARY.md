# Phase 10 Tier 2: DKL Fix Summary

**Date**: October 9, 2025  
**Status**: ‚úÖ DKL Training Fixed, Benchmark Running  
**Time Invested**: ~8 hours debugging + implementation

---

## üéØ Problem Statement

Initial DKL implementation produced **constant predictions** across all test points:
- MVN mean range: [-0.0035, -0.0035] (effectively zero)
- RMSE: 48.68K (worse than random baseline at 34.38K)
- Root cause: Improper GPyTorch ExactGP wiring

---

## üî¨ Root Cause Analysis

### Issue 1: Training Loop
**Problem**: Used `set_train_data()` every epoch with extracted features  
**Why it's wrong**: Breaks GPyTorch's internal conditioning machinery  
**Official pattern**: Pass raw `train_x` to `ExactGP.__init__`, extract features in `forward()`

### Issue 2: Prediction Method
**Problem**: Called `self.forward(X)` for predictions  
**Why it's wrong**: `forward()` computes GP **prior**, not **posterior**  
**Official pattern**: Call `self(X)` in eval mode (uses ExactGP's __call__ with conditioning)

### Issue 3: BoTorch Wrapper
**Problem**: `latent_mvn()` called `self.forward(X)`  
**Why it's wrong**: Same as Issue 2 - returns prior, not conditioned posterior  
**Fix**: Changed to `self(X)` for proper conditioning

---

## ‚úÖ Fixes Implemented

### 1. Proper ExactGP Initialization
```python
# BEFORE (wrong)
with torch.no_grad():
    train_z = feature_extractor(train_x)
super().__init__(train_z, train_y, likelihood)

# AFTER (correct)
super().__init__(train_x, train_y, likelihood)  # Raw X, not features!
```

### 2. Standard Training Loop
```python
# Official GPyTorch DKL pattern
for epoch in range(n_epochs):
    self.train()
    self.likelihood.train()
    optimizer.zero_grad()
    
    with gpytorch.settings.cholesky_jitter(1e-5):
        output = self(self.train_x_original)  # Extracts features in forward()
        loss = -mll(output, self.train_y_original)
    
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
    optimizer.step()
```

### 3. Proper Predictions
```python
# BEFORE (wrong - computes prior)
def predict(self, X_test):
    test_z = self.feature_extractor(X_test)
    mean = self.mean_module(test_z)
    covar = self.covar_module(test_z)
    posterior = MultivariateNormal(mean, covar)
    return posterior

# AFTER (correct - computes conditional posterior)
def predict(self, X_test):
    self.eval()
    self.likelihood.eval()
    with torch.no_grad():
        posterior = self(X_test)  # ExactGP conditions on training data
        predictive = self.likelihood(posterior)
    return predictive.mean, predictive.stddev
```

### 4. Stability Improvements
- **Priors**: Gamma priors on lengthscale (3.0, 6.0) and noise (1.1, 0.05)
- **Constraints**: Noise floor at 1e-6 to prevent collapse
- **Jitter**: Cholesky jitter 1e-5 for numerical stability
- **Optimizer**: Two groups (NN: lr, GP hyperparams: 0.1√ólr)
- **Diagnostics**: Log z.std, lengthscale, noise every 10 epochs

---

## üìä Validation Results

### Diagnostic Test (100 train, 50 test, synthetic data)

| Metric | Before Fix | After Fix | Status |
|--------|------------|-----------|--------|
| **Feature std** | 0.408 | 0.406 | ‚úÖ Similar |
| **MVN mean range** | [-0.0035, -0.0035] | [-6.72, 7.51] | ‚úÖ **VARYING!** |
| **MVN variance range** | [0.695, 0.695] | [0.409, 0.695] | ‚úÖ **VARYING!** |
| **Direct pred range** | [-0.0035, -0.0035] | [-6.72, 7.51] | ‚úÖ **VARYING!** |
| **BoTorch pred range** | [-0.0035, -0.0035] | [-6.72, 7.51] | ‚úÖ **VARYING!** |
| **RMSE** | 11.95K | 12.19K | ‚úÖ Near baseline |
| **Diagnosis** | ‚ùå Constant | ‚úÖ Working | ‚úÖ **FIXED** |

---

## üöÄ Current Status

### Completed ‚úÖ
1. **DKL Model**: Proper ExactGP wiring following official GPyTorch pattern
2. **Training Loop**: Standard DKL training (no set_train_data hacks)
3. **Prediction**: Proper posterior via `self(X)` not `self.forward(X)`
4. **BoTorch Wrapper**: Correct `latent_mvn()` for analytic EI
5. **Diagnostics**: Comprehensive logging (z.std, lengthscale, noise)
6. **Validation**: Synthetic test passes (varying predictions)

### In Progress ‚è≥
- **Tier 2 Benchmark**: Running DKL vs GP vs Random (5 seeds √ó 20 rounds)
  - Expected runtime: 10-15 minutes
  - Will validate DKL learns better than GP/Random on UCI dataset

### Pending üìã
1. Analyze benchmark results
2. Generate evidence pack (plots, metrics, interpretation)
3. Write completion report
4. (Optional) Multi-fidelity BO
5. (Optional) HTSC-2025 benchmark

---

## üîë Key Learnings

### 1. ExactGP Conditioning Machinery
- `forward(x)` computes the GP **prior** (mean + covariance)
- `__call__(x)` in eval mode computes the **conditional posterior** (conditions on train data)
- Training: `loss = -mll(model(train_x), train_y)` uses forward() internally
- Prediction: `model(test_x)` uses ExactGP's conditioning via __call__

### 2. Feature Extraction in DKL
- Initialize ExactGP with **raw inputs**, not features
- Extract features inside `forward()` method
- GPyTorch handles conditioning automatically when you call `model(X)` in eval mode
- Do NOT manually update training data with features each epoch

### 3. BoTorch Integration
- `GPyTorchPosterior` wraps `MultivariateNormal` from GP output
- For analytic EI: Must be single-outcome Gaussian (q=1)
- Posterior must come from `model(X)` (conditioned), not `model.forward(X)` (prior)
- Use `LogExpectedImprovement` instead of `ExpectedImprovement` (better numerics)

---

## üìÅ Files Modified

1. **`phase10_gp_active_learning/models/dkl_model.py`** (105 lines changed)
   - Fixed `__init__`: Pass raw X to ExactGP
   - Fixed `fit()`: Standard training loop, removed set_train_data
   - Fixed `predict()`: Use `self(X)` for conditioning
   - Fixed `latent_mvn()`: Use `self(X)` for BoTorch wrapper
   - Added priors, constraints, diagnostics

2. **`scripts/diagnose_dkl.py`** (97 lines, new)
   - Synthetic test for DKL training
   - Checks feature variance, MVN outputs, RMSE
   - Validates BoTorch wrapper compatibility

3. **`PHASE10_TIER2_DIAGNOSTIC.md`** (261 lines, new)
   - Initial diagnostic report documenting the issue
   - Root cause analysis with hypotheses
   - Troubleshooting steps and recommendations

---

## üéì References

1. **GPyTorch DKL Tutorial**: https://docs.gpytorch.ai/en/v1.13/examples/06_PyTorch_NN_Integration_DKL/
   - Official pattern for Deep Kernel Learning
   - Shows proper ExactGP initialization and training

2. **BoTorch Models Documentation**: https://botorch.org/docs/models/
   - `GPyTorchModel` API requirements
   - `posterior()` method specification

3. **BoTorch Acquisition Functions**: https://botorch.org/docs/acquisition/
   - Analytic vs. Monte Carlo EI
   - `LogExpectedImprovement` for better numerics

---

## üîÆ Next Steps

### Immediate (Tonight)
1. ‚úÖ Wait for benchmark completion (~10-15 min)
2. Analyze results: DKL vs GP vs Random
3. Generate plots and evidence pack
4. Write Tier 2 completion report

### Optional (Phase 3)
1. Multi-Fidelity Bayesian Optimization
   - Low-fidelity: 8 features (fast experiments)
   - High-fidelity: 81 features (expensive experiments)
   - Cost-aware acquisition

2. HTSC-2025 Benchmark
   - Load 140-material dataset from HuggingFace
   - Extract composition features
   - Compare DKL vs GP vs Random

3. Interactive Dashboard
   - Streamlit app for live BO
   - Real-time uncertainty visualization
   - Experiment recommendation UI

---

## üìä Expected Benchmark Results

### Hypotheses
1. **DKL should beat GP**: Learned features capture nonlinear relationships
2. **Both should beat Random**: Active learning selects informative samples
3. **Target**: ‚â•30% RMSE reduction vs Random, p < 0.01

### If DKL doesn't beat GP...
- **Possible causes**:
  - NN architecture too complex (overparameterized)
  - Insufficient training epochs
  - UCI features already well-suited for linear GP (matminer descriptors are physics-based)

- **Diagnostics**:
  - Check z.std across training (should increase)
  - Check lengthscale (should be reasonable, ~1-5)
  - Check noise (should be small, ~0.01-0.1)

- **Fixes**:
  - Simpler NN (8 dims instead of 16)
  - More training epochs (100 instead of 20)
  - Better hyperparameter tuning

---

**Status**: ‚úÖ DKL implementation correct. Waiting for benchmark results to validate performance.

**¬© 2025 GOATnote Autonomous Research Lab Initiative**  
**Contact**: b@thegoatnote.com

