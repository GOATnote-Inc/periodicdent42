# Phase D.3: Fusion Roadmap - 24.22 â†’ 5 Î¼s (4.84Ã— speedup needed)

**Date**: Oct 17, 2025  
**Current**: xFormers CUTLASS @ 24.22 Î¼s (FP16, Flash-style fusion)  
**Target**: < 5 Î¼s  
**Hardware**: L4 (sm_89, Ada)

---

## **ðŸ”¬ Theoretical Ceiling Analysis (Roofline-Style)**

### **Current Kernel (xFormers CUTLASS)**:
```
Algorithm: Flash-style SDPA (fused QK^T â†’ softmax â†’ PV)
Precision: FP16
Fusion level: High (never materializes SÃ—S)
Bytes moved: ~O(SÂ·D) (near-optimal for FP16)
Occupancy: 9.28% (intentional, TC-optimized)
```

### **Theoretical Gains Available**:

| Optimization | Type | Theoretical Max | Realistic | Notes |
|--------------|------|-----------------|-----------|-------|
| **Flash fusion** | Memory | 3-10Ã— | **DONE** | xFormers already has this âœ… |
| **FP8 quantization** | Memory+Compute | 2-3Ã— | **1.5-2.0Ã—** | L4 has FP8 TCs (2Ã— throughput) |
| **cp.async pipelining** | Latency hiding | 1.2-1.5Ã— | **1.1-1.3Ã—** | Overlap HBM â†” compute |
| **Persistent CTAs** | Occupancy | 1.2-1.6Ã— | **1.1-1.4Ã—** | Better for variable S |
| **Block sparsity** | Compute | 1/(density) | **N/A** | Model change required |
| **INT4/INT2** | Memory+Compute | 4-8Ã— | **N/A** | Too aggressive for SDPA |

### **Stackable Combinations** (realistic on L4):
```
Path A: FP8 only
  24.22 Î¼s â†’ 12-16 Î¼s (1.5-2.0Ã—)
  Status: âš ï¸ Miss 5 Î¼s target

Path B: FP8 + cp.async
  24.22 Î¼s â†’ 10-14 Î¼s (1.7-2.4Ã—)
  Status: âš ï¸ Miss 5 Î¼s target

Path C: FP8 + cp.async + persistent-CTA
  24.22 Î¼s â†’ 8-12 Î¼s (2.0-3.0Ã—)
  Status: âš ï¸ Miss 5 Î¼s target

Path D: FP8 + all optimizations + algorithmic innovation
  24.22 Î¼s â†’ 5-8 Î¼s (3.0-4.8Ã—)
  Status: âœ… Possible but requires research
```

**Reality Check**: **Getting to < 5 Î¼s requires 4.84Ã— speedup**  
- Available from known techniques: **~2-3Ã—** (FP8 + pipelining + tuning)
- **Gap: ~1.6-2.4Ã— more needed** â†’ requires novel approach

---

## **ðŸŽ¯ Recommended Path: "Practical Excellence"**

### **Goal**: 8-12 Î¼s (2-3Ã— from current, 3.9-6Ã— faster than SDPA)

**Why This Target**:
- Achievable with known techniques âœ…
- Demonstrates GPU mastery âœ…
- Portfolio-worthy âœ…
- Doesn't require research breakthroughs âœ…

### **Strategy: Stack High-Leverage Optimizations**

---

## **ðŸ“‹ Phase D.3 Action Plan (6-10 hours)**

### **Step 1: FP8 SDPA Kernel** (4-6 hours) â±ï¸

**Goal**: 12-16 Î¼s (1.5-2.0Ã— from 24.22 Î¼s)

**Implementation**:
```cuda
// FP8 E4M3 for Q, K, V (Ada native)
// FP32 accumulators for stability
// Per-channel scales for Q, K, V

__global__ void sdpa_fp8_kernel(
    const __nv_fp8_e4m3* Q,     // Quantized
    const __nv_fp8_e4m3* K,
    const __nv_fp8_e4m3* V,
    const float* Q_scale,        // Per-head scales
    const float* K_scale,
    const float* V_scale,
    half* O,
    int B, int H, int S, int D
) {
    // Flash-style tiling
    // Dequant on load: Q_fp32 = __nv_fp8_to_float(Q) * Q_scale
    // Compute QK^T in FP32
    // Softmax in FP32 (critical for accuracy)
    // PV with dequant-on-load
    // Write FP16 output
}
```

**Acceptance Gates**:
- âœ… Correctness: max_diff â‰¤ 5e-3 (FP8 has more error)
- âœ… Latency: 12-16 Î¼s
- âœ… NCU: 2Ã— FP8 throughput vs FP16 visible

**L4-Specific Details**:
```
FP8 Tensor Core Config (Ada):
  - Tile: 16Ã—16Ã—16 (E4M3)
  - Throughput: 2Ã— vs FP16
  - Accumulator: FP32 (for stability)
  
SMEM Budget: 100 KB
  - Q_tile_fp8: 32Ã—64Ã—1 = 2 KB
  - K_tile_fp8: 64Ã—64Ã—1 = 4 KB
  - V_tile_fp8: 64Ã—64Ã—1 = 4 KB
  - S_tile_fp32: 32Ã—64Ã—4 = 8 KB
  - O_tile_fp32: 32Ã—64Ã—4 = 8 KB
  - Scales: ~1 KB
  Total: ~27 KB âœ… (fits!)
```

---

### **Step 2: cp.async Pipelining** (2-3 hours) â±ï¸

**Goal**: 10-13 Î¼s (1.2-1.3Ã— more from Step 1)

**Implementation**:
```cuda
// Double-buffer K/V tiles with cp.async
// While computing tile N, load tile N+1

#define STAGES 2  // Double buffering

__shared__ __align__(16) __nv_fp8_e4m3 K_smem[STAGES][TILE_N][TILE_K];
__shared__ __align__(16) __nv_fp8_e4m3 V_smem[STAGES][TILE_N][TILE_K];

// Pipeline loop
for (int tile = 0; tile < num_tiles; tile++) {
    int stage = tile % STAGES;
    
    // Async load next tile (if not last)
    if (tile + 1 < num_tiles) {
        int next_stage = (tile + 1) % STAGES;
        __pipeline_memcpy_async(
            &K_smem[next_stage][0][0],
            &K_global[...],
            sizeof(K_smem[0])
        );
        __pipeline_commit();
    }
    
    // Wait for current tile
    __pipeline_wait_prior(STAGES - 1);
    __syncthreads();
    
    // Compute with current tile
    compute_qkt_tile(Q_smem, K_smem[stage], S_tile);
    
    __syncthreads();
}
```

**Acceptance Gates**:
- âœ… Latency: 10-13 Î¼s
- âœ… NCU: "Memory busy" overlaps "Compute busy"
- âœ… Correctness maintained

---

### **Step 3: Persistent CTA + Tuning** (1-2 hours) â±ï¸

**Goal**: 8-11 Î¼s (1.1-1.3Ã— more from Step 2)

**Implementation**:
```cuda
// Launch fewer blocks, let them loop over work
// Better for variable sequence lengths

dim3 grid(NUM_SMS * BLOCKS_PER_SM, 1, 1);  // e.g., 58 SMs Ã— 6 = 348 blocks

__global__ void __launch_bounds__(256, 6)  // 6 blocks/SM for 50% occ
sdpa_persistent_kernel(...) {
    int work_id = blockIdx.x;
    
    // Loop over work items
    while (work_id < total_work) {
        // Process this attention head
        process_attention_head(work_id, ...);
        
        // Get next work item
        work_id += gridDim.x;
    }
}
```

**L4-Specific Tuning**:
```
Target Occupancy: 50% (vs 9.28% current)
  - Blocks/SM: 6
  - Warps/Block: 8 (256 threads)
  - Registers/Thread: 48 (use -maxrregcount=48)
  
Expected: 1.2-1.4Ã— from better tail utilization
```

---

### **Step 4: Micro-Optimizations** (1 hour) â±ï¸

**Quick Wins**:
1. **Warp specialization**: Producer/consumer warps
2. **Bank conflict elimination**: XOR swizzling for SMEM
3. **Instruction reordering**: Hide latency with ILP
4. **Aggressive unrolling**: Small inner loops

**Expected**: 1.05-1.15Ã— cumulative

---

## **ðŸ“Š Expected Results**

### **Conservative Estimate**:
```
Step 0 (Baseline): 24.22 Î¼s
Step 1 (FP8):      16.00 Î¼s  (1.51Ã—)
Step 2 (cp.async): 13.00 Î¼s  (1.23Ã—)
Step 3 (persist):  11.00 Î¼s  (1.18Ã—)
Step 4 (micro):    10.50 Î¼s  (1.05Ã—)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final: 10.50 Î¼s (2.31Ã— total)
```

### **Optimistic Estimate**:
```
Step 0 (Baseline): 24.22 Î¼s
Step 1 (FP8):      13.00 Î¼s  (1.86Ã—)
Step 2 (cp.async): 10.50 Î¼s  (1.24Ã—)
Step 3 (persist):   9.00 Î¼s  (1.17Ã—)
Step 4 (micro):     8.50 Î¼s  (1.06Ã—)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Final: 8.50 Î¼s (2.85Ã— total)
```

**vs SDPA Baseline (47.10 Î¼s)**:
- Conservative: 4.5Ã— faster âœ…
- Optimistic: 5.5Ã— faster âœ…âœ…

**vs Original Target (< 5 Î¼s)**:
- Conservative: âŒ Miss (10.50 Î¼s)
- Optimistic: âš ï¸ Close (8.50 Î¼s)

---

## **ðŸ”¬ Alternative: "Research Mode" (< 5 Î¼s)**

### **What It Takes**:
To hit < 5 Î¼s (4.84Ã— from 24.22 Î¼s), need **novel techniques**:

1. **Algorithmic Innovation**:
   - Linear attention (O(N) vs O(NÂ²))
   - Locality-sensitive hashing (LSH attention)
   - Low-rank factorization

2. **Extreme Quantization**:
   - INT4/INT2 (with calibration)
   - Block-wise quantization
   - Mixed precision (INT8 QK^T, FP8 PV)

3. **Sparsity**:
   - Block-sparse attention (requires model change)
   - Top-K attention (approximate)

**Timeline**: Weeks-months (research project)  
**Risk**: High  
**Recommendation**: âŒ Not for current session

---

## **âœ… Recommended Next Steps**

### **Option 1: "Practical Excellence" (Recommended)**
**Goal**: 8-12 Î¼s (2.5-3Ã— from current)  
**Time**: 6-10 hours  
**Success Rate**: 80%  
**Grade**: A+ (demonstrates GPU mastery)

**Path**:
1. Implement FP8 SDPA (4-6h)
2. Add cp.async pipelining (2-3h)
3. Tune + benchmark (1h)
4. Document results

### **Option 2: "Accept Champion"**
**Current**: 24.22 Î¼s (1.94Ã— vs SDPA)  
**Time**: 0 hours  
**Grade**: A (already excellent)

**Rationale**:
- Champion validated âœ…
- Professional analysis complete âœ…
- 118.5Ã— total speedup âœ…
- Time better spent elsewhere

### **Option 3: "Research Mode"**
**Goal**: < 5 Î¼s (4.84Ã— from current)  
**Time**: Weeks-months  
**Success Rate**: 30%  
**Grade**: A++ if successful, A- if not

**Not recommended** without research backing

---

## **ðŸŽ¯ My Recommendation: Option 1 (FP8 Path)**

**Why**:
- Achievable in one session (6-10 hours) âœ…
- High success rate (80%) âœ…
- Demonstrates advanced GPU optimization âœ…
- 4-5.5Ã— faster than SDPA âœ…
- Portfolio-ready âœ…

**Next Action** (if you choose Option 1):
1. I'll create FP8 SDPA kernel for L4
2. Implement cp.async double buffering
3. Benchmark and validate
4. NCU profile to confirm 2Ã— FP8 throughput

**Next Action** (if you choose Option 2):
1. Update final documentation
2. Create portfolio summary
3. Celebrate 118.5Ã— speedup! ðŸŽ‰

---

**Your Choice**: Which path? ðŸ¤”
- **Type "1"** for FP8 optimization (6-10 hours)
- **Type "2"** to accept champion (0 hours)
- **Type "3"** for research mode (weeks)


