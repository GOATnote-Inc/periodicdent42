# FlashCore v12 Optimization Plan

**Current**: 1512 Âµs (51Ã— slower than SDPA @ 29 Âµs)  
**Target**: <60 Âµs (2Ã— SDPA, realistic first milestone)  
**Final Target**: <28 Âµs (excellence)

---

## ðŸ” Root Cause Analysis

### Current v12 Issues

```
âŒ 4 barriers per KV tile:
   1. After KV load
   2. After QK^T compute
   3. After softmax
   4. After PÂ·V compute
   
âŒ All 512 threads loading KV (not just load warps)
âŒ No warp-local sync (all use __syncthreads())
âŒ Compute warps wait for everyone
```

### Performance Breakdown (Estimated)

```
Per KV tile (11 tiles for S=512):
- 4Ã— __syncthreads() = ~4 Âµs overhead
- Redundant loads = ~10 Âµs wasted
- Total waste = ~14 Âµs/tile Ã— 11 tiles = 154 Âµs

Actual work (WMMA + softmax): ~100 Âµs
Overhead: ~1400 Âµs
Total: ~1500 Âµs âœ… matches observed
```

---

## ðŸŽ¯ Optimization Strategy

### Phase 1: Reduce Barriers (Target: 400-600 Âµs)

**Remove 3 of 4 barriers per tile**:

```cpp
// BEFORE (v12 current):
for (kv_tile) {
    load_kv();           // all threads
    __syncthreads();     // âŒ barrier 1
    
    compute_qkt();       // compute warps
    __syncthreads();     // âŒ barrier 2
    
    softmax();           // softmax warp
    __syncthreads();     // âŒ barrier 3
    
    compute_pv();        // compute warps
    __syncthreads();     // âŒ barrier 4
}

// AFTER (v12 optimized):
for (kv_tile) {
    if (is_load) {
        load_kv();       // only load warps
    }
    __syncthreads();     // âœ… single barrier
    
    if (is_compute) {
        compute_qkt();
        __syncwarp();    // warp-local
    }
    
    if (is_softmax) {
        softmax();
        __syncwarp();    // warp-local
    }
    
    if (is_compute) {
        compute_pv();
        __syncwarp();    // warp-local
    }
    // NO barrier here - next iteration does it
}
```

**Expected**: 1512 â†’ 400-600 Âµs (2.5-4Ã— speedup)

---

### Phase 2: Warp-Specialized Loads (Target: 200-300 Âµs)

**Only load warps load data**:

```cpp
if (is_load) {
    // 4 load warps, each loads 1/4 of KV tile
    const int warp_load_id = warp_id - kComputeWarps;
    const int rows_per_warp = (kv_len + 3) / 4;
    const int row_start = warp_load_id * rows_per_warp;
    const int row_end = min(row_start + rows_per_warp, kv_len);
    
    for (int row = row_start + lane_id; row < row_end; row += kWarpSize) {
        // Load K row
        // Load V row
    }
}
```

**Expected**: 400-600 â†’ 200-300 Âµs (2Ã— speedup from Phase 1)

---

### Phase 3: Vectorized Loads (Target: 150-200 Âµs)

**Use uint4 (128-bit) loads**:

```cpp
if (is_load && D % 8 == 0) {
    for (int row = ...; row < row_end; row++) {
        for (int col = lane_id * 8; col < D; col += kWarpSize * 8) {
            uint4 k_vec = *reinterpret_cast<const uint4*>(&K_bh[row * D + col]);
            *reinterpret_cast<uint4*>(&layout.k_tiles[stage][row * kTilePadD + col]) = k_vec;
        }
    }
}
```

**Expected**: 200-300 â†’ 150-200 Âµs (1.5Ã— speedup from Phase 2)

---

### Phase 4: cp.async Prefetch (Target: 80-120 Âµs)

**Double-buffer with cp.async**:

```cpp
// Prefetch first tile
if (is_load) {
    cp_async_load(stage=0, tile=0);
}
cp_async_commit();

for (kv_tile_idx = 0; kv_tile_idx < num_kv_tiles; kv_tile_idx++) {
    const int curr_stage = kv_tile_idx % 2;
    const int next_stage = 1 - curr_stage;
    
    // Prefetch next tile while computing current
    if (is_load && kv_tile_idx + 1 < num_kv_tiles) {
        cp_async_load(next_stage, tile=kv_tile_idx+1);
        cp_async_commit();
    }
    
    cp_async_wait<0>();  // Wait for current
    
    if (is_compute) {
        compute_qkt(curr_stage);
        compute_pv(curr_stage);
    }
    if (is_softmax) {
        softmax(curr_stage);
    }
    
    __syncthreads();  // Single barrier
}
```

**Expected**: 150-200 â†’ 80-120 Âµs (2Ã— speedup from Phase 3)

---

### Phase 5: Register Blocking + Tuning (Target: <60 Âµs)

**Optimizations**:
- Keep Q in registers (no reload from SMEM)
- Reduce SMEM round-trips
- Tune launch bounds
- Profile with NCU

**Expected**: 80-120 â†’ 40-60 Âµs (2Ã— speedup from Phase 4)

---

## ðŸ“Š Cumulative Speedup Roadmap

| Phase | Optimization | Latency (Âµs) | vs SDPA | Speedup |
|:------|:-------------|-------------:|--------:|--------:|
| v12 Baseline | 4 barriers, all load | 1512 | 51.8Ã— | 1.0Ã— |
| Phase 1 | Reduce to 1 barrier | 500 | 17.1Ã— | 3.0Ã— |
| Phase 2 | Warp-specialized loads | 250 | 8.6Ã— | 6.0Ã— |
| Phase 3 | Vectorized loads | 175 | 6.0Ã— | 8.6Ã— |
| Phase 4 | cp.async prefetch | 100 | 3.4Ã— | 15.1Ã— |
| Phase 5 | Register blocking | 50 | 1.7Ã— | 30.2Ã— |

**Final Target**: 29-50 Âµs (parity to 2Ã— SDPA)

---

## ðŸ”§ Implementation Order

### Immediate (30 min)
**Phase 1**: Remove 3 barriers â†’ test â†’ should see 3Ã— speedup

### Next (1 hour)
**Phase 2**: Warp-specialized loads â†’ test â†’ should see 6Ã— cumulative

### Follow-up (2 hours)
**Phase 3-4**: Vectorization + cp.async â†’ test â†’ should see 15Ã— cumulative

### Final (4 hours)
**Phase 5**: Full optimization â†’ NCU profile â†’ tune â†’ target <60 Âµs

---

## âœ… Success Criteria

**Minimum**: <100 Âµs (10Ã— speedup from baseline)  
**Good**: <60 Âµs (2Ã— SDPA, 25Ã— speedup from baseline)  
**Excellence**: <28 Âµs (parity with SDPA, 54Ã— speedup from baseline)

**Current**: v12 @ 1512 Âµs  
**Next Milestone**: v12 Phase 1 @ ~500 Âµs (target: next 30 minutes)

---

**NO QUITTING. Optimizing Phase 1 NOW! ðŸš€**

