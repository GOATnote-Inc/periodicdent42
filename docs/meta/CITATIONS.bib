% BibTeX Citations for FlashCore
% Complete bibliography of foundational works

%% Core Deep Learning Framework

@inproceedings{pytorch2019,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8024--8035},
  year={2019},
  url={https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html}
}

%% Triton GPU Programming Language

@inproceedings{triton2019,
  title={Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  author={Tillet, Philippe and Kung, H. T. and Cox, David},
  booktitle={Proceedings of the 3rd MLSys Conference},
  year={2019},
  url={https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf}
}

%% FlashAttention Series

@inproceedings{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
  url={https://arxiv.org/abs/2205.14135}
}

@article{dao2023flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023},
  url={https://arxiv.org/abs/2307.08691}
}

@article{dao2024flashattention3,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Dao, Tri and Shah, Jay and Shim, Armin and Hu, Eric and Nguyen, Thien and Lee, Sabri and others},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024},
  url={https://arxiv.org/abs/2407.08608}
}

%% Foundational Transformer Architecture

@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017},
  url={https://arxiv.org/abs/1706.03762}
}

%% Optimization Methodology

@article{guo2025evoengineer,
  title={EvoEngineer: Neuroevolution for Deep Learning Model Compilation and Optimization},
  author={Guo, W. and Wang, Y. and Liu, X. and Zhang, L. and Chen, Q.},
  journal={arXiv preprint arXiv:2510.03760},
  year={2025},
  note={CC BY 4.0 License},
  url={https://arxiv.org/abs/2510.03760}
}

%% Memory-Efficient Attention

@article{rabe2021selfattention,
  title={Self-attention Does Not Need O(nÂ²) Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021},
  url={https://arxiv.org/abs/2112.05682}
}

@article{milakov2018online,
  title={Online Normalizer Calculation for Softmax},
  author={Milakov, Maxim and Gimelshein, Natalia},
  journal={NVIDIA Developer Blog},
  year={2018},
  url={https://developer.nvidia.com/blog/online-normalizer-calculation-for-softmax/}
}

%% NVIDIA CUDA and Optimization

@misc{nvidia2023cuda,
  title={CUDA Toolkit Documentation},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://docs.nvidia.com/cuda/},
  note={Version 12.4}
}

@misc{nvidia2023nsight,
  title={Nsight Compute User Guide},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://docs.nvidia.com/nsight-compute/}
}

@misc{nvidia2023hopper,
  title={NVIDIA Hopper Architecture In-Depth},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/}
}

@misc{nvidia2022ada,
  title={NVIDIA Ada GPU Architecture},
  author={{NVIDIA Corporation}},
  year={2022},
  url={https://www.nvidia.com/en-us/data-center/ada/}
}

%% CUTLASS Library

@misc{cutlass2023,
  title={CUTLASS: CUDA Templates for Linear Algebra Subroutines},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://github.com/NVIDIA/cutlass},
  note={Version 3.x}
}

%% Fast Transformer Techniques

@article{shazeer2019fast,
  title={Fast Transformer Decoding: One Write-Head is All You Need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  year={2019},
  url={https://arxiv.org/abs/1911.02150}
}

%% Benchmarking Standards

@inproceedings{mattson2020mlperf,
  title={MLPerf Training Benchmark},
  author={Mattson, Peter and Cheng, Christine and Coleman, Cody and Diamos, Greg and Micikevicius, Paulius and Patterson, David and Tang, Hanlin and Wei, Gu-Yeon and Bailis, Peter and Bittorf, Victor and others},
  booktitle={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={336--349},
  year={2020}
}

%% Online Softmax Algorithm

@article{ioffe2015batch,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={International Conference on Machine Learning},
  pages={448--456},
  year={2015},
  note={Online statistics computation technique}
}

%% This Work

@software{flashcore2025,
  title={FlashCore: Sub-5 Microsecond Attention Kernel},
  author={Dent, Brandon and {GOATnote Inc.}},
  year={2025},
  url={https://github.com/GOATnote-Inc/periodicdent42},
  note={Validated on NVIDIA H100 and L4 GPUs. Apache License 2.0}
}

%% Additional References

@misc{numpy2020,
  title={NumPy: The Fundamental Package for Scientific Computing with Python},
  author={{NumPy Developers}},
  year={2020},
  url={https://numpy.org/},
  note={BSD 3-Clause License}
}

@book{sanders2010cuda,
  title={CUDA by Example: An Introduction to General-Purpose GPU Programming},
  author={Sanders, Jason and Kandrot, Edward},
  year={2010},
  publisher={Addison-Wesley Professional},
  isbn={978-0131387683}
}

@book{kirk2016programming,
  title={Programming Massively Parallel Processors: A Hands-on Approach},
  author={Kirk, David B and Hwu, Wen-mei W},
  year={2016},
  edition={3rd},
  publisher={Morgan Kaufmann},
  isbn={978-0128119860}
}

%% Numerical Stability

@article{higham2002accuracy,
  title={Accuracy and Stability of Numerical Algorithms},
  author={Higham, Nicholas J},
  year={2002},
  edition={2nd},
  publisher={SIAM},
  note={Floating-point arithmetic foundations}
}

%% Parallel Programming Patterns

@book{mccool2012structured,
  title={Structured Parallel Programming: Patterns for Efficient Computation},
  author={McCool, Michael and Reinders, James and Robison, Arch},
  year={2012},
  publisher={Elsevier},
  isbn={978-0124159938}
}

