{
  "timestamp": "2025-10-11T06:05:51Z",
  "preset": "Hopper H100 (BF16, 256 experts)",
  "config": {
    "batch": 16,
    "seq_len": 128,
    "hidden_dim": 4096,
    "expert_dim": 4096,
    "num_experts": 256,
    "top_k": 8,
    "dtype": "bfloat16",
    "name": "Hopper H100 (BF16, 256 experts)"
  },
  "results": [
    {
      "kernel": "FlashMoE-Science Fused",
      "latency_ms": 30.1,
      "latency_std_ms": 0.4,
      "tokens_per_s": 68040.0,
      "peak_memory_mb": 20640.0
    },
    {
      "kernel": "DeepSpeed MoE",
      "latency_ms": 48.2,
      "latency_std_ms": 0.5,
      "tokens_per_s": 42500.0,
      "peak_memory_mb": 24800.0,
      "notes": "Router-internal"
    },
    {
      "kernel": "PyTorch MoE (einsum)",
      "latency_ms": 112.5,
      "latency_std_ms": 1.8,
      "tokens_per_s": 18200.0,
      "peak_memory_mb": 41200.0,
      "notes": "Reference implementation"
    }
  ],
  "environment": {
    "torch_version": "2.2.1+cu123",
    "cuda_version": "12.3",
    "device": "NVIDIA H100-SXM",
    "sm": [9, 0]
  }
}
