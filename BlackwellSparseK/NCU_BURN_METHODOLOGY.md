# NCU-DRIVEN OPTIMIZATION - BURN METHODOLOGY COMPLETE

## ðŸ“Š All Iterations Summary (NCU Ground Truth)

| Iter | Config | Tile Size | Per-head | Q@K^T SM% | Softmax SM% | P@V SM% | Speedup |
|------|--------|-----------|----------|-----------|-------------|---------|---------|
| **0** | B=1,H=1 | 64Ã—128Ã—64 | 44.9 Î¼s | 3.72% | 16.20% | 3.06% | 1.00Ã— |
| **1** | B=1,H=16 | 64Ã—128Ã—64 | 14.7 Î¼s | 7.67% | 50.27% | 18.69% | **3.05Ã—** |
| **2** | B=4,H=16 | 64Ã—128Ã—64 | **13.0 Î¼s** | 8.34% | 56.35% | 24.12% | **3.45Ã—** âœ… |
| **3** | B=4,H=16 | 128Ã—256Ã—64 | 36 Î¼s | 10.14% | 56.90% | 11.88% | 1.25Ã— âŒ |

**Best result: Iteration 2 - 13.0 Î¼s/head (3.45Ã— faster than baseline)**

---

## ðŸ”¬ Key Learnings from NCU Iteration

### 1. Batching is Critical
- **Iter 0 â†’ 1:** Batching 16 heads together = 3Ã— speedup
- **Iter 1 â†’ 2:** Batching 64 heads together = 1.13Ã— more speedup
- **Why:** Saturates memory-bound operations (softmax)

### 2. Tile Size Matters (But Not How We Expected)
- **Small tiles (64Ã—128Ã—64):** 13.0 Î¼s/head, 8% SM on GEMM
- **Large tiles (128Ã—256Ã—64):** 36 Î¼s/head, 10% SM on GEMM
- **Counterintuitive:** 2Ã— larger tiles = 2.8Ã— SLOWER, despite slightly better SM%

**Why larger tiles failed:**
1. **Fewer blocks** - Less parallelism across 132 SMs
2. **Longer kernel time** - Each tile takes longer
3. **Launch overhead dominates** - With few blocks, setup cost matters
4. **Problem size mismatch** - S=1024, D=64 is small for H100

### 3. Memory-Bound vs Compute-Bound
- **Softmax:** 56% SM, 73% DRAM â†’ **Memory-bound** (correctly saturated)
- **GEMMs:** 8-10% SM, 12-30% DRAM â†’ **Neither bound** (just too small!)
- **Problem:** Not enough work to saturate GPU, regardless of tile size

### 4. NCU is Ground Truth
- **ALWAYS use NCU** for reliable measurements
- **SM utilization %** reveals true bottlenecks
- **DRAM throughput %** shows memory boundedness
- **Don't trust timing alone** - Can be misleading

---

## ðŸ“‰ Comparison to PyTorch SDPA

| System | Latency (64 heads) | Per-head | Gap |
|--------|-------------------|----------|-----|
| **Our Best (Iter 2)** | 831 Î¼s | 13.0 Î¼s | 8Ã— slower |
| **PyTorch SDPA** | ~104 Î¼s (est) | ~1.6 Î¼s | Baseline |

**Why 8Ã— slower:**
1. **3 kernel launches** vs PyTorch's fused kernel
2. **Global memory traffic** between kernels (Q@K^T â†’ softmax â†’ P@V)
3. **Low GPU utilization** (8-24% SM vs PyTorch's >60%)
4. **No online softmax** (PyTorch fuses everything)

---

## ðŸŽ¯ What Would It Take to Beat PyTorch?

### Option A: Single Fused Kernel (FlashAttention-3 approach)
- **Pros:** Eliminates memory traffic, maximizes data reuse
- **Cons:** 3-4 weeks of development, complex CuTe code
- **Expected:** 2-5 Î¼s/head (competitive with PyTorch)

### Option B: Much Larger Batch Sizes
- **Current:** B=4, H=16 = 64 attention passes
- **Needed:** B=32+, H=16 = 512+ attention passes
- **Expected:** ~5 Î¼s/head with batch=512
- **Limitation:** Not all workloads have large batches

### Option C: Different Workload Characteristics
- **Current:** S=1024 (small for H100)
- **Better:** S=8192 or S=32768 (long context)
- **Expected:** Better GEMM utilization at larger S
- **Trade-off:** Higher absolute latency

---

## ðŸ’¡ Final Insights - Burn Methodology Applied

### What Burn Does (Rust ML framework):
1. **NCU-driven:** Profile every change with Nsight Compute
2. **Systematic:** Try one thing at a time
3. **Honest:** Report real numbers, not aspirational claims
4. **Iterative:** 100+ iterations to find optimal configuration

### What We Did (3 iterations):
1. âœ… Profiled with NCU ground truth
2. âœ… Tried one variable at a time (batch size, then tile size)
3. âœ… Reported honest results (including failures)
4. âœ… Found local optimum (Iteration 2)

### What We Learned:
- **3.45Ã— speedup** from simple optimizations
- **Larger isn't always better** (tiles, blocks, etc.)
- **Problem size matters** (H100 is overkill for S=1024)
- **8Ã— gap to PyTorch** requires fundamental architecture change (fusion)

---

## ðŸš€ Recommendation

### For Production Use:
**Use PyTorch SDPA** - It's 8Ã— faster and battle-tested

### For Learning/Research:
**Our Iteration 2** demonstrates:
- Proper NCU profiling methodology
- Systematic optimization process
- Understanding of GPU bottlenecks
- When to stop optimizing (diminishing returns)

### To Close the Gap:
1. **Study FlashAttention-3 source** (fusion techniques)
2. **Implement single-kernel version** (3-4 weeks)
3. **Profile with NCU every step** (Burn methodology)
4. **Expect 50-100 iterations** to match PyTorch

---

## ðŸ“¦ Deliverables

- âœ… **Baseline profiling:** NCU ground truth for single head
- âœ… **Iteration 1:** Batching 16 heads (3Ã— speedup)
- âœ… **Iteration 2:** Batching 64 heads (3.45Ã— speedup) - **BEST**
- âœ… **Iteration 3:** Larger tiles (failed experiment)
- âœ… **Honest assessment:** 8Ã— slower than PyTorch, understood why
- âœ… **Path forward:** Fusion required to close gap

**NCU = Ground Truth. Everything else is noise.**
