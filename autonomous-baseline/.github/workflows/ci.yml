name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  test:
    name: Test Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.10", "3.11", "3.12", "3.13"]
        include:
          - os: macos-latest
            python-version: "3.13"
          - os: windows-latest
            python-version: "3.13"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -e .[dev]
      
      - name: Run linters
        run: |
          ruff check src/ tests/
          mypy src/ --ignore-missing-imports
      
      - name: Run tests with coverage
        run: |
          pytest tests/ -v --cov=src --cov-report=xml --cov-report=term
      
      - name: Upload coverage to Codecov
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.13'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  acceptance:
    name: Acceptance Tests
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -e .[dev]
      
      - name: Run acceptance tests
        run: |
          pytest tests/ -v -m acceptance || true
      
      - name: Generate evidence pack
        run: |
          python -c "
          from src.reporting import generate_manifest
          from pathlib import Path
          artifacts_dir = Path('artifacts/ci_test')
          artifacts_dir.mkdir(parents=True, exist_ok=True)
          (artifacts_dir / 'test.txt').write_text('CI test artifact')
          manifest = generate_manifest(artifacts_dir)
          print(f'Generated manifest with {len(manifest[\"artifacts\"])} artifacts')
          "
      
      - name: Upload evidence pack
        uses: actions/upload-artifact@v4
        with:
          name: evidence-pack-${{ github.sha }}
          path: artifacts/ci_test/

  coverage-gate:
    name: Coverage Gate (≥86%)
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install -e .[dev]
      
      - name: Run coverage check
        run: |
          pytest tests/ --cov=src --cov-report=term --cov-fail-under=86
      
      - name: Generate coverage badge
        if: github.ref == 'refs/heads/main'
        run: |
          coverage-badge -o coverage.svg -f

  reproducibility:
    name: Reproducibility Check
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      
      - name: Run reproducibility test (Run 1)
        run: |
          python -c "
          import numpy as np
          from src.data.splits import LeakageSafeSplitter
          import pandas as pd
          
          # Generate synthetic data
          np.random.seed(42)
          data = pd.DataFrame({
              'formula': [f'Element{i}' for i in range(100)],
              'Tc': np.random.uniform(10, 100, 100),
          })
          
          # Split with fixed seed
          splitter = LeakageSafeSplitter(random_state=42, enforce_near_dup_check=False)
          splits1 = splitter.split(data, 'formula', 'Tc')
          
          # Save checksums
          import hashlib
          checksums1 = {
              'train': hashlib.sha256(splits1['train']['Tc'].values.tobytes()).hexdigest(),
              'val': hashlib.sha256(splits1['val']['Tc'].values.tobytes()).hexdigest(),
              'test': hashlib.sha256(splits1['test']['Tc'].values.tobytes()).hexdigest(),
          }
          
          with open('checksums_run1.txt', 'w') as f:
              for split, checksum in checksums1.items():
                  f.write(f'{split}: {checksum}\n')
          
          print('Run 1 checksums:', checksums1)
          "
      
      - name: Run reproducibility test (Run 2)
        run: |
          python -c "
          import numpy as np
          from src.data.splits import LeakageSafeSplitter
          import pandas as pd
          
          # Generate synthetic data (same seed)
          np.random.seed(42)
          data = pd.DataFrame({
              'formula': [f'Element{i}' for i in range(100)],
              'Tc': np.random.uniform(10, 100, 100),
          })
          
          # Split with same seed
          splitter = LeakageSafeSplitter(random_state=42, enforce_near_dup_check=False)
          splits2 = splitter.split(data, 'formula', 'Tc')
          
          # Save checksums
          import hashlib
          checksums2 = {
              'train': hashlib.sha256(splits2['train']['Tc'].values.tobytes()).hexdigest(),
              'val': hashlib.sha256(splits2['val']['Tc'].values.tobytes()).hexdigest(),
              'test': hashlib.sha256(splits2['test']['Tc'].values.tobytes()).hexdigest(),
          }
          
          with open('checksums_run2.txt', 'w') as f:
              for split, checksum in checksums2.items():
                  f.write(f'{split}: {checksum}\n')
          
          print('Run 2 checksums:', checksums2)
          "
      
      - name: Compare checksums
        run: |
          echo "=== Run 1 Checksums ==="
          cat checksums_run1.txt
          echo ""
          echo "=== Run 2 Checksums ==="
          cat checksums_run2.txt
          echo ""
          echo "=== Comparing checksums ==="
          diff checksums_run1.txt checksums_run2.txt && echo "✅ Data splitting reproducibility verified!" || exit 1
      
      - name: Model training reproducibility (Run 1)
        run: |
          python -c "
          import numpy as np
          import pickle
          import hashlib
          from src.models import RandomForestQRF
          
          # Generate synthetic training data
          np.random.seed(42)
          X_train = np.random.randn(100, 10)
          y_train = X_train[:, 0] * 2 + X_train[:, 1] * 3 + np.random.randn(100) * 0.5
          
          # Train model with fixed seed
          model = RandomForestQRF(n_estimators=10, random_state=42)
          model.fit(X_train, y_train)
          
          # Get predictions (should be deterministic)
          X_test = np.random.randn(10, 10)
          y_pred1, y_lower1, y_upper1 = model.predict_with_uncertainty(X_test)
          
          # Save prediction checksum
          checksum = hashlib.sha256(y_pred1.tobytes()).hexdigest()
          with open('model_predictions_run1.txt', 'w') as f:
              f.write(f'predictions: {checksum}\n')
          
          print(f'Run 1 model checksum: {checksum}')
          "
      
      - name: Model training reproducibility (Run 2)
        run: |
          python -c "
          import numpy as np
          import pickle
          import hashlib
          from src.models import RandomForestQRF
          
          # Generate same synthetic training data
          np.random.seed(42)
          X_train = np.random.randn(100, 10)
          y_train = X_train[:, 0] * 2 + X_train[:, 1] * 3 + np.random.randn(100) * 0.5
          
          # Train model with same seed
          model = RandomForestQRF(n_estimators=10, random_state=42)
          model.fit(X_train, y_train)
          
          # Get predictions (should be identical)
          X_test = np.random.randn(10, 10)
          y_pred2, y_lower2, y_upper2 = model.predict_with_uncertainty(X_test)
          
          # Save prediction checksum
          checksum = hashlib.sha256(y_pred2.tobytes()).hexdigest()
          with open('model_predictions_run2.txt', 'w') as f:
              f.write(f'predictions: {checksum}\n')
          
          print(f'Run 2 model checksum: {checksum}')
          "
      
      - name: Compare model predictions
        run: |
          echo "=== Model Prediction Checksums ==="
          cat model_predictions_run1.txt
          cat model_predictions_run2.txt
          echo ""
          echo "=== Verifying model reproducibility ==="
          diff model_predictions_run1.txt model_predictions_run2.txt && echo "✅ Model training reproducibility verified!" || exit 1

  leakage-check:
    name: Leakage Detection
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      
      - name: Run leakage detection test
        run: |
          python -c "
          import numpy as np
          from src.data.splits import LeakageSafeSplitter
          from src.guards.leakage_checks import LeakageDetector
          import pandas as pd
          
          # Generate synthetic data
          np.random.seed(42)
          data = pd.DataFrame({
              'formula': [f'Element{i}' for i in range(100)],
              'Tc': np.random.uniform(10, 100, 100),
          })
          
          # Split data
          splitter = LeakageSafeSplitter(random_state=42, enforce_near_dup_check=False)
          splits = splitter.split(data, 'formula', 'Tc')
          
          # Check for leakage
          detector = LeakageDetector()
          overlap = detector.check_formula_overlap(
              splits['train'], splits['test'], 'formula'
          )
          
          if overlap:
              print(f'❌ Leakage detected: {len(overlap)} overlapping formulas')
              exit(1)
          else:
              print('✅ No formula overlap detected')
          "

  calibration-check:
    name: Calibration Validation
    runs-on: ubuntu-latest
    needs: test
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
      
      - name: Run calibration test
        run: |
          python -c "
          import numpy as np
          from src.models import RandomForestQRF
          from src.uncertainty.calibration_metrics import (
              prediction_interval_coverage_probability,
              expected_calibration_error,
          )
          
          # Generate synthetic data
          np.random.seed(42)
          X_train = np.random.randn(200, 10)
          y_train = X_train[:, 0] * 2 + X_train[:, 1] * 3 + np.random.randn(200) * 0.5
          X_test = np.random.randn(100, 10)
          y_test = X_test[:, 0] * 2 + X_test[:, 1] * 3 + np.random.randn(100) * 0.5
          
          # Train model
          model = RandomForestQRF(n_estimators=50, random_state=42)
          model.fit(X_train, y_train)
          
          # Get predictions with uncertainty
          y_pred, y_lower, y_upper = model.predict_with_uncertainty(X_test)
          y_std = model.get_epistemic_uncertainty(X_test)
          
          # Compute metrics
          picp = prediction_interval_coverage_probability(y_test, y_lower, y_upper)
          ece = expected_calibration_error(y_test, y_pred, y_std, n_bins=10)
          
          print(f'PICP: {picp:.3f} (target: 0.94-0.96)')
          print(f'ECE: {ece:.3f} (target: ≤0.05)')
          
          # Check targets (relaxed for synthetic data)
          if picp < 0.80 or picp > 1.0:
              print(f'❌ PICP out of range: {picp:.3f}')
              exit(1)
          if ece > 0.2:
              print(f'❌ ECE too high: {ece:.3f}')
              exit(1)
          
          print('✅ Calibration check passed')
          "

  docs:
    name: Documentation Build
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Check documentation files
        run: |
          echo "Checking required documentation files..."
          
          required_files=(
            "README.md"
            "docs/OVERVIEW.md"
            "docs/RUNBOOK.md"
            "docs/GO_NO_GO_POLICY.md"
            "docs/PHYSICS_JUSTIFICATION.md"
          )
          
          missing_files=()
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Missing: $file"
              missing_files+=("$file")
            else
              echo "✅ Found: $file"
            fi
          done
          
          if [ ${#missing_files[@]} -gt 0 ]; then
            echo "❌ Missing ${#missing_files[@]} required documentation files"
            exit 1
          else
            echo "✅ All required documentation files present"
          fi
      
      - name: Check documentation links
        run: |
          echo "Checking for broken internal links..."
          # Simple grep for markdown links
          grep -r "\[.*\](.*.md)" docs/ README.md || echo "No internal links found"

  notify:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [test, acceptance, coverage-gate, reproducibility, leakage-check, calibration-check, docs]
    if: failure()
    
    steps:
      - name: Notify failure
        run: |
          echo "❌ CI/CD pipeline failed!"
          echo "Please check the failed jobs above for details."
