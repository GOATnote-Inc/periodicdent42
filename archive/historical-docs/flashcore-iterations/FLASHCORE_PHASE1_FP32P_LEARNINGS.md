# FlashCore Phase 1 FP32 P - Critical Learnings

**Date**: October 22, 2025  
**Phase**: Phase 1 (Fix Error with FP32 P)  
**Result**: ‚ùå **BLOCKED by SMEM constraints with 32√ó32 tiles**  
**Decision**: ‚úÖ **Pivot to Phase 2A (64√ó64 tiles + FP32 P)**

---

## üéØ **What We Attempted**

### **Goal**
- Fix error: 0.51 ‚Üí <0.10
- Method: FP32 P matrix (4√ó more precision than FP16)
- Keep: 32√ó32 tiles

### **Implementation**
```cpp
// Attempted SMEM layout:
sQ:        5 KB
sKT:       5 KB
sV:        5 KB
sS_f32:    4 KB  // FP32 scores
sP:        4 KB  // FP32 probabilities 
sP_fp16:   2 KB  // Conversion buffer
m/l_smem:  0.25 KB
U_smem:    10 KB
sU_part:   4 KB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:     39.25 KB ‚Üí **52 KB with alignment**
```

**Problem**: 52 KB > 48 KB default SMEM limit!

---

## üî¥ **Why Buffer Reuse Failed**

### **Attempt 1: Union**
```cpp
__shared__ union {
    float as_scores[32][32];  // 4 KB
    float as_probs[32][32];   // 4 KB
} score_prob;
```

**Result**: GPU runtime error, 4KB stack frame  
**Cause**: Compiler allocated union on stack instead of shared memory

---

### **Attempt 2: Alias**
```cpp
__shared__ float sS_f32[32][32];  // 4 KB
#define sP sS_f32  // Same buffer
```

**Result**: Data corruption  
**Root Cause**: 
```cpp
// In softmax loop:
for (int n = 0; n < kv_len; ++n) {
    float s = sS_f32[m][n];        // READ score
    float p = expf(s - m_new);
    sP[m][n] = p;                   // WRITE prob to SAME BUFFER
}
// Later iterations read corrupted scores!
```

**Why it fails**:
- Need to read ALL scores for a row to compute exp values
- But we write probabilities to the same buffer as we go
- This overwrites scores we haven't read yet
- Result: Later softmax values use corrupted data

---

## üí° **Key Insight: Timing of Buffer Reuse**

### **What Works**
Buffer reuse works when usage is **temporally separated**:
```cpp
// Phase 1: Use buffer A
compute_and_store_to_A();
__syncthreads();

// Phase 2: Use buffer B (same memory)
read_from_A_and_write_to_B();  // OK if we don't need A anymore
```

### **What Doesn't Work**
Buffer reuse fails when we need **simultaneous access**:
```cpp
// Same loop iteration:
for (int n = 0; n < N; ++n) {
    float x = buffer_A[n];     // READ from A
    float y = compute(x);
    buffer_A[n] = y;           // WRITE to same A - CORRUPTS DATA!
}
```

---

## üìä **SMEM Arithmetic**

### **32√ó32 Tiles: Constrained**
```
Minimum needed for FP32 P:
- Q, K, V tiles:      15 KB
- Scores (FP32):      4 KB
- Probs (FP32):       4 KB  ‚Üê Can't reuse with scores!
- FP16 buffer:        2 KB
- Stats (m, l):       0.25 KB
- Output (U):         10 KB
- Partials:           4 KB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:                39.25 KB ‚Üí 52 KB aligned ‚ùå
```

### **64√ó64 Tiles: Room to Grow**
```
With 64√ó64 tiles:
- Q, K, V tiles:      30 KB (2√ó larger)
- Union {
    Scores (FP32):    16 KB
    Probs (FP32):     16 KB  ‚Üê NOW union works! (temporal separation)
  }
- FP16 buffer:        8 KB
- Stats (m, l):       0.5 KB
- Output (U):         20 KB
- Partials:           16 KB
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:                90.5 KB ‚úÖ Fits in 96KB with union!
```

**Why union works with 64√ó64**:
- Full QK matmul completes ‚Üí stores to `scores`
- `__syncthreads()` ensures all warps done
- Softmax phase reads `scores`, writes to `probs`
- After softmax, don't need `scores` anymore
- **Temporal separation achieved!**

---

## üéì **Lessons Learned**

### **1. Unions are Fragile**
- May allocate on stack instead of SMEM
- Use with extreme caution
- Test thoroughly for runtime errors

### **2. Buffer Reuse Requires Temporal Separation**
- Can't reuse if reading AND writing in same loop
- Need `__syncthreads()` between usage phases
- Carefully analyze data dependencies

### **3. SMEM is Precious at Small Tiles**
- 32√ó32 tiles = 48KB limit is TIGHT
- FP32 precision costs 2√ó space
- Need larger tiles for advanced optimizations

### **4. Larger Tiles Unlock Optimizations**
- 64√ó64 tiles: 2√ó block work, 2√ó SMEM, BUT
- 90KB total still fits in 96KB L4 limit
- Enables FP32 P + union + better occupancy

### **5. Compile-time vs Runtime SMEM**
- Compiler checks against 48KB by default
- `cudaFuncSetAttribute` is runtime call
- Must fit in compile-time limit OR use `launch_bounds` wisely

---

## ‚úÖ **Why Phase 2A (64√ó64 Tiles) is the Right Pivot**

### **Benefits**
1. **Fixes Error**: Room for FP32 P with proper union
2. **Improves Performance**: 4√ó more work per block
3. **Natural Progression**: Was planned anyway
4. **Higher Confidence**: Kills two birds with one stone

### **Expected Results**
```
Performance: 279 ‚Üí 110-140 Œºs (2-2.5√ó speedup)
Error:       0.51 ‚Üí 0.05-0.10 (10√ó improvement)
SMEM:        90 KB (fits in 96KB with union)
Registers:   ~100-110 (acceptable)
Confidence:  80-85%
```

### **Implementation Strategy**
1. Use user-provided `flashcore_fused_wmma_64x64.cu`
2. Union for `scores ‚Üî probs` (works with temporal separation)
3. 8 warps (4√ó2 grid)
4. Opt-in to 96KB SMEM
5. Test & validate

---

## üö¶ **Next Steps**

### **Immediate** (Next 30 min)
- [ ] Copy user-provided 64√ó64 kernel
- [ ] Create build script
- [ ] Create test script
- [ ] Deploy to L4

### **Phase 2A Validation** (Next 2-3 hours)
- [ ] Compile successfully
- [ ] Verify SMEM usage (~90KB)
- [ ] Check register count (~100-110)
- [ ] Test correctness (expect <0.10 error)
- [ ] Benchmark performance (expect ~120 Œºs)

### **If Phase 2A Succeeds**
- [ ] Proceed to Phase 2B (cp.async)
- [ ] Expected: 120 ‚Üí 50-60 Œºs
- [ ] Then Phase 2C (micro-opts)
- [ ] Final: <40 Œºs target! üéØ

---

## üìà **Revised Confidence Levels**

| Metric | Original (Phase 1 alone) | Revised (Phase 2A) |
|--------|--------------------------|-------------------|
| **Error** | 80% for <0.10 | **85%** for <0.10 |
| **Performance** | No change (285 Œºs) | **90%** for 110-140 Œºs |
| **Both Goals** | 80% error only | **80%** for both! |

**Why higher confidence?**
- Larger tiles = proven technique
- Union works with temporal separation
- Natural fit for FP32 P
- Performance boost is bonus!

---

## üéØ **Bottom Line**

### **What We Learned**
‚úÖ FP32 P is the right solution for error  
‚úÖ 32√ó32 tiles too constrained for FP32 P  
‚úÖ Buffer reuse needs temporal separation  
‚úÖ 64√ó64 tiles unlock the optimization

### **What's Next**
‚úÖ Phase 2A: 64√ó64 tiles + FP32 P  
‚úÖ Expected: Error <0.10, Performance 110-140 Œºs  
‚úÖ Confidence: 80-85%

### **Path Forward**
```
Current:     279 Œºs, 0.51 error
             ‚Üì Phase 2A (2-3 hours)
Phase 2A:    120 Œºs, 0.08 error  ‚Üê Both fixed!
             ‚Üì Phase 2B (2-3 hours)
Phase 2B:    55 Œºs, 0.08 error
             ‚Üì Phase 2C (1 hour)
Phase 2C:    38 Œºs, 0.08 error   ‚Üê TARGET ACHIEVED! üéØ
```

**Total time**: 5-7 hours  
**Success probability**: 75-80%

---

**Status**: ‚úÖ **Ready for Phase 2A implementation!**  
**Confidence**: üî• **HIGH** - This is the right path!

**Let's build something excellent!** üöÄ

