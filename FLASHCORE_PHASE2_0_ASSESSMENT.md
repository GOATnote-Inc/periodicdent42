# FlashCore Phase 2.0 Assessment - Dynamic SMEM 64Ã—64 Tiles

**Date**: October 23, 2025, 03:00 PST  
**Status**: âœ… **Dynamic SMEM Working** | âš ï¸ **Performance Slower Than Expected**  
**Branch**: `feat/stage5-warp-spec-persistent`

---

## ğŸ¯ Phase 2.0 Goal & Results

### Goal
Implement 64Ã—64 tiles with dynamic SMEM to unlock path to <40 Î¼s

### Results

**Correctness**: âœ… **PERFECT**
```
Max Error: 0.000244 (same as Phase 1.3!)
Mean Error: 0.000013
Status: All tests PASS âœ…
```

**Performance**: âš ï¸ **SLOWER**
```
Phase 1.3 (32Ã—32 static):  131.24 Î¼s  â† Baseline
Phase 2.0 (64Ã—64 dynamic): 145.79 Î¼s  â† 11% SLOWER!
PyTorch SDPA:               28.23 Î¼s  (reference)

Speedup vs Phase 1.3: 0.90Ã— (regression!)
Gap to SDPA: 5.16Ã— slower
```

---

## ğŸ”¬ Root Cause Analysis

### Why is 64Ã—64 Slower?

#### 1. Occupancy Reduction (Primary Cause)
```
Metric                  32Ã—32 Tiles    64Ã—64 Tiles
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SMEM per block:         ~38 KB         ~82 KB
Max CTAs per SM:        2              1           â† HALF!
Total CTAs (S=512):     16             4           â† 4Ã— FEWER!
Active warps per SM:    16             8           â† HALF!
```

**Impact**: With 82 KB SMEM, only 1 CTA can fit per SM (vs 2 for 32Ã—32)
- L4 has 58 SMs
- 32Ã—32: 16 CTAs Ã— can utilize many SMs
- 64Ã—64: 4 CTAs Ã— underutilizes GPU (most SMs idle!)

#### 2. Work Distribution
```
Sequence length: 512
32Ã—32 tiles: 512/32 = 16 tiles per sequence
64Ã—64 tiles: 512/64 = 8 tiles per sequence

With B=1, H=8:
32Ã—32: 1Ã—8Ã—16 = 128 total CTAs (good parallelism)
64Ã—64: 1Ã—8Ã—8  = 64 total CTAs (half the parallelism)
```

**Impact**: 64 CTAs across 58 SMs means some SMs get 1-2 CTAs, others sit idle part of the time.

#### 3. Missing Optimizations
The 64Ã—64 kernel still uses:
- âŒ Scalar loads (no vectorization)
- âŒ Sequential memory access
- âŒ No warp specialization
- âŒ No producer/consumer pipeline

**Impact**: Larger tiles need complementary optimizations to pay off!

---

## ğŸ’¡ Key Insights (EvoEngineer Learning)

### 1. Larger Tiles â‰  Always Faster

**Conventional Wisdom**: Larger tiles â†’ fewer launches â†’ faster
**Reality**: Larger tiles â†’ less occupancy â†’ can be slower!

**The Tradeoff**:
```
Small tiles (32Ã—32):
âœ… High occupancy (2 CTAs/SM)
âœ… Better parallelism
âŒ More kernel launches
âŒ More synchronization

Large tiles (64Ã—64):
âŒ Low occupancy (1 CTA/SM)
âŒ Less parallelism
âœ… Fewer launches
âœ… More work per CTA
```

**Lesson**: Need to balance tile size with occupancy!

### 2. Dynamic SMEM is Working âœ…

**Success**: We successfully bypassed the 48 KB static SMEM limit!

```cpp
// What we achieved:
extern __shared__ char smem[];  // Dynamic allocation
cudaFuncSetAttribute(kernel, 
    cudaFuncAttributeMaxDynamicSharedMemorySize, 82*1024);
```

**Value**: This unlocks experimentation with different tile sizes (48Ã—48, 64Ã—64, etc.)

### 3. Vectorization is Critical

**Hypothesis**: 64Ã—64 will become faster once we add:
1. **Vectorized loads** (float4 â†’ 8Ã— halfs per transaction)
2. **cp.async pipeline** (overlap compute + memory)
3. **Warp specialization** (producer/consumer roles)

**Expected**: 146 Î¼s â†’ ~60-80 Î¼s with full optimizations

---

## ğŸ“Š Comparison Matrix

| Metric | Phase 1.3 (32Ã—32) | Phase 2.0 (64Ã—64) | Ideal |
|--------|-------------------|-------------------|-------|
| **Correctness** | 0.000244 | 0.000244 | âœ… Same |
| **Latency** | 131 Î¼s | 146 Î¼s | âŒ Slower |
| **SMEM/block** | 38 KB | 82 KB | - |
| **CTAs/SM** | 2 | 1 | âš ï¸ Half |
| **Warps/SM** | 16 | 8 | âš ï¸ Half |
| **Total CTAs** | 128 | 64 | âš ï¸ Half |
| **Vectorization** | âŒ No | âŒ No | Need! |
| **cp.async** | âŒ No | âŒ No | Need! |
| **Warp spec** | âŒ No | âŒ No | Need! |

---

## ğŸ¯ Path Forward

### Option A: Optimize 32Ã—32 First (RECOMMENDED)

**Rationale**: Better occupancy, already proven fast

**Plan**:
1. Add vectorization to Phase 1.3 (32Ã—32 kernel)
2. Add cp.async pipeline
3. Target: 131 Î¼s â†’ ~60 Î¼s
4. Then compare with optimized 64Ã—64

**Expected**: Faster path to <40 Î¼s goal

### Option B: Optimize 64Ã—64 Aggressively

**Rationale**: Committed to 64Ã—64, add all optimizations

**Plan**:
1. Add vectorization to Phase 2.0 (64Ã—64 kernel)
2. Add warp specialization
3. Add cp.async pipeline
4. Hope optimizations overcome occupancy loss

**Risk**: May still be slower due to occupancy

### Option C: Hybrid Auto-Tuning (BEST)

**Rationale**: Let data decide optimal tile size

**Plan**:
1. Implement vectorization as a module
2. Apply to both 32Ã—32 and 64Ã—64
3. Also try 48Ã—48 (intermediate)
4. Use EvoEngineer loop to find best config

**Expected**: Optimal tile size may be 48Ã—48 or even 32Ã—32!

---

## ğŸ§  EvoEngineer Recommendations

### Immediate Next Steps (4-6 hours)

**Phase 2.1: Vectorized I/O** (apply to 32Ã—32)
```
Target: 131 Î¼s â†’ ~80 Î¼s (1.6Ã— speedup)

Changes:
- float4 loads for Q/K/V (8 halfs = 16 bytes)
- Vectorized stores for output
- Coalesced memory access patterns

Expected:
- 1.3-1.6Ã— speedup from memory bandwidth
- Lower latency than current 64Ã—64
```

**Phase 2.2: cp.async Pipeline** (on vectorized 32Ã—32)
```
Target: ~80 Î¼s â†’ ~50 Î¼s (1.6Ã— speedup)

Changes:
- Double-buffer K/V tiles
- Overlap compute (tile N) with prefetch (tile N+1)
- Use cp.async.cg.shared.global

Expected:
- Hide memory latency
- Better SM utilization
```

**Phase 2.3: Tile Size Sweep** (auto-tune)
```
Target: Find optimal tile (may be 32Ã—32, 48Ã—48, or 64Ã—64)

Test configurations:
- 32Ã—32 (current best)
- 48Ã—48 (middle ground)
- 64Ã—64 (if vectorization helps enough)

Choose based on:
- Actual measured latency
- Occupancy metrics from NCU
- Tensor Core utilization
```

---

## ğŸ“ˆ Realistic Expectations

### Can We Hit <40 Î¼s?

**Current Status**:
```
Phase 1.3 (baseline): 131 Î¼s
Gap to 40 Î¼s:         3.3Ã— speedup needed
Gap to SDPA (28 Î¼s):  4.7Ã— speedup needed
```

**With Vectorization** (Phase 2.1):
```
Expected: 131 Î¼s â†’ ~80 Î¼s (1.6Ã— from memory BW)
Remaining: 2.0Ã— to <40 Î¼s
```

**With cp.async** (Phase 2.2):
```
Expected: ~80 Î¼s â†’ ~50 Î¼s (1.6Ã— from overlap)
Remaining: 1.25Ã— to <40 Î¼s
```

**With Micro-Optimization** (Phase 2.3):
```
Expected: ~50 Î¼s â†’ ~40 Î¼s (1.25Ã— from tuning)
Target: <40 Î¼s âœ… (borderline achievable)
```

### Confidence Levels

| Target | Confidence | Notes |
|--------|------------|-------|
| <80 Î¼s | 90% | Vectorization is proven |
| <60 Î¼s | 75% | cp.async well-documented |
| <50 Î¼s | 60% | Requires good tuning |
| <40 Î¼s | 40% | Very challenging, may need all tricks |

**Reality**: PyTorch SDPA (28 Î¼s) is **extremely** well-optimized. Getting within 1.5Ã— (42 Î¼s) would be excellent for a custom kernel!

---

## âœ… Phase 2.0 Success Criteria

| Criterion | Target | Achieved | Status |
|-----------|--------|----------|--------|
| **Dynamic SMEM** | Working | Yes | âœ… PASS |
| **64Ã—64 Tiles** | Working | Yes | âœ… PASS |
| **Correctness** | < 1e-3 | 0.000244 | âœ… PASS |
| **<80 Î¼s** | Yes | 146 Î¼s | âŒ FAIL |
| **Faster than Phase 1.3** | Yes | No | âŒ FAIL |

**Overall**: 3/5 criteria met. Technical success (dynamic SMEM works), but performance needs optimization.

---

## ğŸ“ Learnings for EvoEngineer Loop

### What We Validated âœ…

1. **Dynamic SMEM works perfectly**
   - Can use 82 KB on L4
   - Manual buffer layout successful
   - No alignment issues

2. **64Ã—64 correctness maintained**
   - Same 0.000244 error as 32Ã—32
   - Warp layout correct (4Ã—4)
   - Online softmax working

3. **Occupancy matters more than expected**
   - 2 CTAs/SM vs 1 CTA/SM = 11% performance difference
   - Need to measure, not assume

### What We Learned ğŸ§ 

1. **Larger tiles need complementary optimizations**
   - Can't just increase tile size
   - Must add vectorization + pipeline
   - Occupancy loss must be overcome

2. **32Ã—32 is surprisingly good**
   - High occupancy (2 CTAs/SM)
   - Good parallelism (128 CTAs total)
   - May be optimal with vectorization!

3. **Auto-tuning is essential**
   - Can't predict optimal tile size
   - Must measure on actual hardware
   - 48Ã—48 might be sweet spot

---

## ğŸš€ Recommended Action Plan

### Immediate (Next 4 hours)

1. âœ… **Accept Phase 2.0 results** as valuable data
2. âœ… **Pivot to vectorizing 32Ã—32** (Phase 2.1)
3. âœ… **Add cp.async** to vectorized 32Ã—32 (Phase 2.2)
4. âœ… **Measure** actual performance gains

### Then (Next 2-4 hours)

5. **Try vectorization on 64Ã—64** to see if it helps
6. **Implement 48Ã—48** as middle ground
7. **Auto-tune** to find optimal tile size
8. **Profile with NCU** to guide optimizations

### Goal

**Target: <50 Î¼s realistic, <40 Î¼s stretch**

**Strategy**: Vectorization first (proven technique), then tune tile size based on data.

---

## ğŸ“ Conclusion

**Phase 2.0: Technical Success, Performance Learning**

âœ… **Achieved**:
- Dynamic SMEM working (82 KB)
- 64Ã—64 tiles implemented
- Perfect correctness maintained
- Valuable occupancy insights

âš ï¸ **Discovered**:
- 64Ã—64 alone is slower (146 Î¼s vs 131 Î¼s)
- Occupancy reduction is significant
- Need vectorization + pipeline for gains
- 32Ã—32 may be optimal with right optimizations

ğŸ¯ **Next**: Vectorize 32Ã—32 kernel (Phase 2.1) for fastest path to <40 Î¼s

---

**EVOENG INEER MODE: Learning from data, adapting strategy!** ğŸ”¥  
**Excellence through measurement, not assumption!** ğŸ“

---

**Last Updated**: October 23, 2025, 03:00 PST  
**Status**: Phase 2.0 complete, pivoting to vectorization strategy  
**Next**: Phase 2.1 - Vectorized I/O on 32Ã—32 tiles

