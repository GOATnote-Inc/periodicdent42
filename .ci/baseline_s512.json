{
  "config": {
    "batch": 32,
    "heads": 8,
    "seq": 512,
    "dim": 64,
    "backend": "auto",
    "dtype": "float16",
    "iterations": 100,
    "warmup": 20
  },
  "statistics": {
    "median_ms": 0.321,
    "mean_ms": 0.329,
    "std_ms": 0.024,
    "cv": 0.074,
    "ci95_ms": [0.3195, 0.3379],
    "p50": 0.321,
    "p95": 0.344,
    "p99": 0.495,
    "min_ms": 0.315,
    "max_ms": 0.521,
    "n_samples": 100
  },
  "performance": {
    "throughput_gflops": 53516.0,
    "bandwidth_gb_s": 113.57
  },
  "memory": {
    "peak_mb": 18.3,
    "allocated_mb": 16.8,
    "reserved_mb": 20.0
  },
  "environment": {
    "gpu": "NVIDIA L4",
    "driver": "570.172.08",
    "cuda": "12.1",
    "pytorch": "2.2.1+cu121",
    "dtype": "float16",
    "tf32": false,
    "deterministic": true,
    "date": "2025-10-14T00:00:00Z"
  },
  "metadata": {
    "source": "baseline_comprehensive.py",
    "commit": "a9af30a",
    "description": "PyTorch SDPA (FlashAttention-2) baseline on L4",
    "notes": "Established October 14, 2025. Used as reference for â‰¥10% speedup gate."
  }
}
