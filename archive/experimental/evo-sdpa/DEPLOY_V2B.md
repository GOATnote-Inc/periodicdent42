# Child-V2b Deployment & Testing Guide

**Created**: Oct 18, 2025  
**Status**: Ready for GPU testing  

---

## ğŸ¯ What's in Child-V2b

### Core Fixes from V2

1. **FIX: Single-Warp Ownership**
   - Each warp owns `M/NUM_WARPS` consecutive rows
   - No inter-warp races on `(m,l)` stats
   - Stats updated within warp, synced via `__syncthreads()`

2. **FIX: Legal cp.async**
   - 16-byte aligned copies with `cp.async.cg.shared.global`
   - Proper `__cvta_generic_to_shared` address conversion
   - Correct `commit_group` + `wait_group<N>` sequence

3. **FIX: SMEM Layout**
   - Added `S_scores` buffer for WMMA output (16.4 KB)
   - Padding (PAD=8) for bank conflict avoidance
   - Total SMEM validated against device limit

4. **ARCHITECTURE**
   ```
   Warps 0-6: Compute (each owns contiguous rows)
   Warp 7:    Producer (cp.async prefetch)
   
   Per K-tile:
     1. Load Q (all warps)
     2. cp.async K/V (warp 7)
     3. Compute Q@K^T (warps 0-6, per-row)
     4. Softmax update (warps 0-6, per-row)
     5. Accumulate P@V (warps 0-6, per-row)
     6. __syncthreads() â† SINGLE sync point
   ```

5. **SMEM Budget** (d=64, STAGES=2)
   ```
   sQ:       64 Ã— 72 Ã— 2 =  9.2 KB
   sK:     2Ã—64 Ã— 72 Ã— 2 = 18.4 KB
   sV:     2Ã—64 Ã— 72 Ã— 2 = 18.4 KB
   S_scores: 64 Ã— 64 Ã— 4 = 16.4 KB
   O_accum:  64 Ã— 72 Ã— 4 = 18.4 KB
   m,l:      64 Ã— 8     =  0.5 KB
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Total:                 ~81 KB < 96 KB âœ…
   ```

### Current Implementation

- **Scalar path for correctness** (not full WMMA yet)
- Focus on streaming softmax correctness first
- TODO: Layer in WMMA after validation

---

## ğŸš€ Quick Deploy (GPU)

### Step 1: Sync code to GPU

```bash
# From local machine:
cd /Users/kiteboard/periodicdent42
git push

# On GPU (cudadent42-l4-dev):
cd ~/periodicdent42
git pull
```

### Step 2: Activate environment

```bash
# On GPU:
cd ~/periodicdent42
source venv/bin/activate  # or your venv path
```

### Step 3: Run acceptance tests

```bash
cd evo-sdpa/bench
python test_v2b.py
```

**Expected Output:**
```
Building Child-V2b...
âœ… Build successful

CHILD-V2b ACCEPTANCE TESTS
========================================================
âœ… B=1 H=8 L= 512 d= 64 causal=0 | custom=XXX.XXÎ¼s torch=XXX.XXÎ¼s speedup=X.XXÃ— max_diff=X.XXXXXX
âœ… B=1 H=8 L= 512 d= 64 causal=1 | custom=XXX.XXÎ¼s torch=XXX.XXÎ¼s speedup=X.XXÃ— max_diff=X.XXXXXX
...
========================================================

SUMMARY: 5/5 tests passed
âœ… ALL ACCEPTANCE TESTS PASSED!
```

### Step 4: Parse resource usage

```bash
# During build, ptxas output is captured
# Extract register/SMEM usage:
cd evo-sdpa/bench
python parse_ptxas.py v2b_test_output.txt
```

**Expected:**
```
PTXAS RESOURCE USAGE
============================================================
Kernel 1:
  Registers:    48-64
  SMEM:         81.0 KB (82944 bytes)
```

---

## ğŸ§ª Acceptance Criteria

### 1. Correctness (MUST PASS)
- âœ… 5/5 test cases pass (`max_diff â‰¤ 1e-3`)
  - `(B=1, H=8, L=512, d=64, causal=False)`
  - `(B=1, H=8, L=512, d=64, causal=True)`
  - `(B=2, H=8, L=2048, d=64, causal=False)`
  - `(B=2, H=8, L=2048, d=64, causal=True)`
  - `(B=2, H=8, L=2048, d=128, causal=False)`

### 2. Performance (DESIRABLE)
- âš ï¸ Faster than PyTorch SDPA on `(1,8,512,64)` (current: likely NO)
- âš ï¸ 10Ã— faster than scalar baseline ~1400 Î¼s (current: likely NO)

**Why not yet?** V2b prioritizes **correctness** over performance:
- Still using scalar dot products
- Not fully exploiting WMMA
- cp.async present but not optimized

### 3. Resource Sanity (MUST PASS)
- âœ… Registers â‰¤ 72/thread
- âœ… SMEM â‰¤ 96 KB/CTA
- âœ… No CUDA errors (invalid config, OOM, etc.)

---

## ğŸ“Š Expected Results & Next Steps

### Scenario 1: âœ… ALL TESTS PASS (Correctness 100%)

**Victory!** The core algorithmic fix worked:
- Single-warp ownership eliminated races
- Streaming softmax math is correct
- Ready to layer WMMA

**Next: Implement Full WMMA (D.3)**
```bash
# Create Child-V2c from V2b
# Focus: Replace scalar dot products with actual WMMA
# Expected: 3-7Ã— speedup from Tensor Cores
```

### Scenario 2: âŒ SOME TESTS FAIL

**Debug strategy:**
1. Check which shapes fail (d=64 vs d=128? causal?)
2. Add `printf` debugging to kernel for failing shape
3. Verify warp ownership logic (rows per warp)
4. Check SMEM bounds and indexing

**Tools:**
```bash
# Add debug prints to sdpa_fused_v2b.cu
# Recompile with -G (device debug)
# Use cuda-gdb if needed
```

### Scenario 3: ğŸ”¥ CUDA ERRORS (Launch failures)

**Common causes:**
- SMEM overflow (check ptxas output)
- Invalid grid/block config
- Dynamic SMEM not set properly

**Fix:**
1. Reduce `N` if SMEM > 96 KB
2. Check `cudaFuncSetAttribute` in dispatcher
3. Validate grid dimensions

---

## ğŸ“ˆ Performance Trajectory

### Current State (estimated)
```
V1 (scalar baseline):    1378 Î¼s (44Ã— slower than PyTorch)
V2b (correctness-first): ~1200 Î¼s (scalar, but clean)
PyTorch SDPA:             ~31 Î¼s (target to beat)
CUTLASS baseline:         ~24 Î¼s (current champion)
```

### Roadmap to < 100 Î¼s

**Phase D.3: Full WMMA** (4-6 hours)
- Replace scalar Q@K^T with 16Ã—16Ã—16 MMA tiles
- Replace scalar P@V with WMMA
- Expected: 300-500 Î¼s (3-4Ã— from V2b)

**Phase D.4: NCU Profiling + I3** (2-3 hours)
- Extract tensor core utilization, DRAM %, bank conflicts
- Generate optimization insights (I3)
- Expected: Identify next 2Ã— lever

**Phase D.5: Elite Loop** (3-4 hours)
- Try 2-lever combinations from I3
- Iterate Top-K=3 best kernels
- Expected: 100-200 Î¼s (best case)

**Total time to ~100 Î¼s: 10-15 hours** (realistic)  
**Total time to < 5 Î¼s: Weeks-months** (research)

---

## ğŸ”§ Development Commands

### Build only (no test)
```bash
cd evo-sdpa/bench
python -c "from bench_sdpa import build_ext; build_ext()"
```

### Single test case (verbose)
```bash
python -c "
from bench_sdpa import build_ext, run_case
import torch
mod = build_ext()
run_case(mod, B=1, H=8, L=512, d=64, causal=False, verbose=True)
"
```

### Check for CUDA errors
```bash
# Add error checking to kernel:
cudaError_t err = cudaGetLastError();
if (err != cudaSuccess) {
    printf(\"CUDA Error: %s\\n\", cudaGetErrorString(err));
}
```

---

## ğŸ“š Key Files

- `kernels/sdpa_fused_v2b.cu` - Main kernel implementation
- `kernels/runtime.hpp` - Dispatcher
- `kernels/sdpa_fused_bindings.cpp` - PyTorch bindings
- `bench/test_v2b.py` - Acceptance tests
- `bench/bench_sdpa.py` - Core benchmark harness
- `bench/parse_ptxas.py` - Resource usage parser

---

## ğŸ“ Design Rationale

### Why Single-Warp Ownership?

**Problem (V2)**: Multiple warps computing same row â†’ race on `m,l`
**Solution (V2b)**: Each warp owns contiguous rows â†’ no races

```cuda
// V2 (BROKEN): All warps computed all rows
const int my_q_row = warp_id % num_q_rows;  // âŒ Race!

// V2b (FIXED): Each warp owns disjoint rows
const int rows_per_warp = (M + NUM_WARPS - 1) / NUM_WARPS;
const int my_row_start = warp_id * rows_per_warp;
const int my_row_end = min(my_row_start + rows_per_warp, num_q_rows);
// âœ… No overlap
```

### Why Scalar Path First?

**EvoEngineer philosophy**: Correctness â†’ Performance
- Get streaming softmax math right (m,l invariants)
- Validate against PyTorch reference
- THEN layer in WMMA optimizations

**Evidence**: V2 had WMMA "framework" but 0% correctness  
V2b has scalar compute but targeting 100% correctness

### Why cp.async Now?

**Foundation for future speedup**:
- Double-buffering K/V tiles (hide latency)
- Warp specialization (producer/consumer)
- Prerequisite for 3-stage pipeline (Lâ‰¥2048)

Even if scalar compute is slow, the *structure* is right.

---

## âœ… Definition of Done (V2b)

**MINIMAL** (Gate to proceed):
- [ ] Builds without errors
- [ ] Launches without CUDA errors
- [ ] 5/5 acceptance tests pass (correctness)

**DESIRABLE** (Nice to have):
- [ ] Faster than V1 baseline (1378 Î¼s â†’ <1000 Î¼s)
- [ ] Registers â‰¤ 64/thread (better occupancy)
- [ ] SMEM â‰¤ 80 KB (more headroom)

**OUT OF SCOPE** (V2c+):
- [ ] Beat PyTorch SDPA (requires WMMA)
- [ ] < 100 Î¼s (requires NCU tuning)
- [ ] < 5 Î¼s (requires research)

---

**Last Update**: Oct 18, 2025  
**Next**: Deploy to GPU and run `test_v2b.py`



