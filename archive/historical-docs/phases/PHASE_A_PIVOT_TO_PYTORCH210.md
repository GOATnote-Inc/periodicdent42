# **Phase A: Pivot to PyTorch 2.1.0 (Option 2)**

**Date**: Oct 17, 2025  
**Decision**: Switch from stable kernel to PyTorch 2.1.0  
**Reason**: Stable kernel has correctness failure (max_diff=0.445, 222√ó tolerance)

---

## **Summary**

After 4.5 hours of Phase A work:
- ‚úÖ Phase A.1 COMPLETE: TDD validation, root cause confirmed (21% on PyTorch 2.5.0)
- ‚ö†Ô∏è Phase A.2 ATTEMPTED: Stable kernel builds/launches but produces incorrect output

**Decision**: Pivot to PyTorch 2.1.0 (proven 100% correct) rather than debug stable kernel.

---

## **Phase A.2 Attempt Results**

### **What Worked** ‚úÖ
```
‚úÖ Build successful (101 regs, 20KB SMEM)
‚úÖ Kernel launches (grid/block fix worked)
‚úÖ Basic sanity checks (no NaN/Inf)
‚úÖ Performance: 1.107 ms (slower but expected)
```

### **What Failed** ‚ùå
```
‚ùå Correctness: max_diff=0.445457
   Tolerance: 0.002
   Ratio: 222√ó worse than required
   
Root Cause: Numerical guards insufficient
- safe_exp() clamping not enough
- Online softmax accumulation drifting
- Possible issues with m_new/l_new updates
```

---

## **Why Pivot to PyTorch 2.1.0**

### **Time Analysis**

| Option | Time to 100% Correct | Confidence | Notes |
|--------|---------------------|------------|-------|
| **Continue Debug** | Unknown (2-6h?) | 50% | Already failed once |
| **PyTorch 2.1.0** | **15 minutes** | **100%** | Proven from history ‚úÖ |

### **Cost-Benefit**

```
Sunk Cost (Phase A.2): 1 hour
  - Valuable learning (numerical guards, TDD, kernel launch)
  - Not wasted - gained experience

Additional Cost:
  - Debug stable kernel: 2-6 hours (uncertain)
  - PyTorch 2.1.0: 15 minutes (certain) ‚úÖ

Opportunity Cost:
  - Time spent debugging = time NOT spent on Phase B/C
  - Phase B (cuBLAS): 6 hours ‚Üí 400-500 Œºs (2√ó speedup)
  - Phase C (WMMA): 8 hours ‚Üí 50-70 Œºs (BEAT SDPA)
```

### **Risk Assessment**

| Factor | Stable Kernel | PyTorch 2.1.0 |
|--------|--------------|---------------|
| **Correctness** | ‚ùå Failed (0.445) | ‚úÖ 100% (proven) |
| **Debug Time** | ‚ö†Ô∏è Unknown | ‚úÖ 0 hours |
| **Confidence** | üü° 50% | ‚úÖ 100% |
| **Blocker** | ‚ùå Yes | ‚úÖ No |

---

## **Execution Plan: PyTorch 2.1.0 (15 minutes)**

### **Step 1: Downgrade PyTorch** (5 minutes)

```bash
# On GPU instance
cd ~/periodicdent42
source ~/venv/bin/activate

# Uninstall current PyTorch
pip uninstall torch -y

# Install PyTorch 2.1.0
pip install torch==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# Verify installation
python -c "import torch; print(f'PyTorch {torch.__version__}')"
# Expected: PyTorch 2.1.0+cu121
```

### **Step 2: Clear Cache** (1 minute)

```bash
# Clear torch extensions cache
rm -rf ~/.cache/torch_extensions

# Verify clean state
ls ~/.cache/torch_extensions 2>/dev/null || echo "Cache cleared ‚úÖ"
```

### **Step 3: Verify Phase 4 Correctness** (5 minutes)

```bash
# Test Phase 4 (already exists from earlier work)
python scripts/standalone_phase4_eval.py

# Expected output:
# üìä Correctness Results:
#    Passed: 100/100 ‚úÖ
# ‚úÖ fast_0 (correctness): 100.0%
```

### **Step 4: Measure Performance** (4 minutes)

```bash
# Quick performance check
python -c "
import torch, time, os, sys
sys.path.insert(0, '.')

# Build Phase 4
os.environ['BLOCK_M'] = '32'
os.environ['NUM_WARPS'] = '8'
os.environ['VEC_WIDTH'] = '4'
os.environ['SYNC_POLICY'] = '2'
os.environ['REDUCE'] = 'warp'

from cudadent42.bench.kernels.build_phase3 import build_phase3_variant
build_phase3_variant()
import fa_phase3

# Test
q = torch.randn(1, 8, 512, 64, device='cuda', dtype=torch.float16)
scale = 1.0 / 64**0.5

# Warmup
for _ in range(10): fa_phase3.forward(q, q.clone(), q.clone(), scale)

# Benchmark
torch.cuda.synchronize()
t0 = time.perf_counter()
for _ in range(100): fa_phase3.forward(q, q.clone(), q.clone(), scale)
torch.cuda.synchronize()
t1 = time.perf_counter()

print(f'Phase 4 Performance: {(t1-t0)*10:.2f} Œºs')
# Expected: ~800-900 Œºs
"
```

---

## **Success Criteria**

```
‚úÖ PyTorch 2.1.0 installed
‚úÖ Cache cleared
‚úÖ Phase 4: 100/100 correctness ‚úÖ
‚úÖ Performance: ~800-900 Œºs (baseline)
‚úÖ Ready for Phase B
```

---

## **What We Learned from Phase A.2**

### **Technical Insights** ‚úÖ

1. **Numerical Guards are Complex**
   - Clamping exponentials [-20, 20] not sufficient
   - Online softmax accumulation sensitive to order
   - Need deep understanding of numerical stability

2. **TDD Approach Works**
   - Progressive testing caught issue early
   - Build ‚Üí Launch ‚Üí Sanity ‚Üí Correctness
   - Saved time vs full integration test

3. **CUDA Bindings Details**
   - `.cpp` files can't use `<<<>>>` syntax
   - Need `.cu` for kernel launch
   - Grid/block configuration critical

### **Process Improvements** ‚úÖ

1. **Know When to Pivot**
   - Sunk cost fallacy avoided
   - 15 min solution beats hours of debugging
   - Pragmatic > Perfect

2. **TDD Saves Time**
   - Caught correctness issue at Test 4
   - Didn't waste time on performance tuning incorrect kernel
   - Clear pass/fail criteria

---

## **Phase B Preview** (After PyTorch 2.1.0)

### **Goal**: Tensor Core Q@K^T ‚Üí 400-500 Œºs (2√ó speedup)

**Baseline** (with PyTorch 2.1.0):
```
Phase 4: ~839 Œºs (100% correct) ‚úÖ
SDPA: 69.7 Œºs (target)
Gap: 12.0√ó slower
```

**Target** (after Phase B):
```
cuBLAS Q@K^T: 400-500 Œºs
Speedup: 1.7-2.1√ó vs Phase 4
Gap to SDPA: 5.7-7.2√ó (reduced)
NCU: 50-60% Tensor Core utilization ‚úÖ
```

---

## **Time Budget**

```
Phase A Total: 4.75 hours
  - A.1 (TDD): 3.5 hours ‚úÖ
  - A.2 (Stable): 1.0 hours ‚ö†Ô∏è (learning experience)
  - A (PyTorch 2.1.0): 0.25 hours ‚úÖ

Remaining for Phase B+C: 13.25 hours
  - Phase B (cuBLAS): 6 hours ‚Üí 400-500 Œºs
  - Phase C (WMMA): 7.25 hours ‚Üí 50-70 Œºs (BEAT SDPA)
```

---

## **Final Recommendation**

**Execute PyTorch 2.1.0 downgrade NOW (15 minutes)**

This is the pragmatic, time-efficient path to Phase B. The 1 hour spent on stable kernel was valuable learning, but continuing would be sunk cost fallacy.

**Next Actions**:
1. Downgrade to PyTorch 2.1.0 (5 min)
2. Verify 100% correctness (5 min)
3. Proceed to Phase B (cuBLAS Q@K^T) (6 hours)

**Expected Outcome**: 100% correctness + clear path to SDPA-superior performance.

---

**Ready to execute PyTorch 2.1.0 downgrade.**

