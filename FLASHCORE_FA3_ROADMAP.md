# FlashCore FA-3: Roadmap to <40 Î¼s

**Date**: October 22, 2025  
**Current Status**: Correctness âœ…, Performance âŒ (2812 Î¼s)  
**Target**: <40 Î¼s (beat PyTorch SDPA's 44 Î¼s)

---

## âœ… **Progress So Far**

### **Simplified Kernel (Current)**
- **Correctness**: âœ… PERFECT (error 0.000244)
- **Performance**: 2812 Î¼s
- **Registers**: 37
- **Issue**: Scalar dot products (no Tensor Cores)

**This proves**: Online softmax algorithm is correct!

---

## ðŸŽ¯ **Optimization Plan**

### **Step 2: WMMA for Tensor Cores** â† **NEXT** (Expected: 2812 â†’ 200-300 Î¼s)

**Why**: Tensor Cores are 10-20Ã— faster than scalar ops for matrix multiply

**Changes**:
1. Replace scalar QÂ·K^T with WMMA 16Ã—16Ã—16 tiles
2. Keep online softmax in registers (already working)
3. Use WMMA for PÂ·V as well

**Expected**: ~200-300 Î¼s (10Ã— faster)

---

### **Step 3: Reduce Synchronization** (Expected: 200-300 â†’ 100-150 Î¼s)

**Why**: Too many `__syncthreads()` hurt performance

**Changes**:
1. Load K/V cooperatively but sync less often
2. Use warp-level sync where possible
3. Overlap loads with compute

**Expected**: ~100-150 Î¼s (2Ã— faster)

---

### **Step 4: Tune Tile Sizes** (Expected: 100-150 â†’ 50-80 Î¼s)

**Why**: Larger tiles amortize overhead

**Changes**:
1. Try M_TILE=128, N_TILE=128
2. Ensure enough shared memory
3. Profile with NCU to find sweet spot

**Expected**: ~50-80 Î¼s (2Ã— faster)

---

### **Step 5: Micro-optimizations** (Expected: 50-80 â†’ <40 Î¼s)

**Why**: Last 20-30% needs fine-tuning

**Changes**:
1. Vectorized loads (float4)
2. Loop unrolling
3. Instruction-level optimizations
4. Maybe add double-buffering if needed

**Expected**: <40 Î¼s âœ…

---

## ðŸ“Š **Performance Targets**

| Step | Description | Expected Latency | vs PyTorch (44 Î¼s) |
|------|-------------|------------------|-------------------|
| **Current** | Simple kernel | 2812 Î¼s | 64Ã— slower âŒ |
| **Step 2** | + WMMA | 200-300 Î¼s | 5-7Ã— slower âš ï¸ |
| **Step 3** | + Less sync | 100-150 Î¼s | 2-3Ã— slower âš ï¸ |
| **Step 4** | + Tune tiles | 50-80 Î¼s | 1.1-1.8Ã— slower âš ï¸ |
| **Step 5** | + Micro-opts | **<40 Î¼s** | **Faster!** âœ… |

---

## ðŸ’¡ **Key Insights**

1. âœ… **Online softmax algorithm is correct** (proven with simple kernel)
2. ðŸŽ¯ **Tensor Cores are the key** (10-20Ã— speedup expected)
3. â±ï¸ **Synchronization overhead matters** (reduce `__syncthreads()`)
4. ðŸ“ **Tile size tuning is critical** (larger = better amortization)
5. ðŸ”§ **Micro-optimizations for last mile** (vectorization, unrolling)

---

## ðŸš€ **Next Action**

Implement **WMMA version** of the kernel:
- Use proven patterns from our earlier WMMA kernels
- Keep the working online softmax logic
- Target: 200-300 Î¼s (10Ã— faster than simple kernel)

**Timeline**: 1-2 hours to implement and test

---

**Status**: Ready to add WMMA and achieve 10Ã— speedup! ðŸš€

