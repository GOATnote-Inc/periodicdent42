NCU BURN-STYLE ITERATIONS 0-8 SUMMARY
=======================================

Iter 0: B=1,H=1, 64×128×64 → 44.9 μs/head (3.7% SM GEMM) BASELINE
Iter 1: B=1,H=16, 64×128×64 → 14.7 μs/head (7.7% SM GEMM) 3.05× faster
Iter 2: B=4,H=16, 64×128×64 → 13.0 μs/head (8.3% SM GEMM) 3.45× faster ✅
Iter 3: B=4,H=16, 128×256×64 → 36 μs/head (10.1% SM GEMM) FAILED - large tiles
Iter 4: B=8,H=16, 64×128×64 → 12.8 μs/head (8.3% SM GEMM) 3.51× faster
Iter 5: B=4,H=16, S=2048 → 50 μs/head (8.4% SM GEMM) FAILED - long seq
Iter 6: B=4,H=16, cluster 2×1×1 → 13.7 μs/head (9.4% SM GEMM) marginal
Iter 7: B=4,H=16, D=128 → 14 μs/head (8.5% SM GEMM) no help
Iter 8: B=4,H=16, persistent → 12.5 μs/head (7.9% SM GEMM) 3.59× faster ✅ NEW BEST

KEY FINDING: GEMM SM% stuck at 7-10% regardless of:
- Batch size (1→128 heads)
- Tile size (64×128→128×256)  
- Sequence length (1024→2048)
- Head dimension (64→128)
- Clustering (1×1→2×1)
- Schedule (WarpSpec→Persistent)

CONCLUSION: Problem size fundamentally too small for H100.
Need fusion or different approach to close 8× gap to PyTorch.

Current best: 12.5 μs/head (Iter 8) vs PyTorch ~1.6 μs/head
