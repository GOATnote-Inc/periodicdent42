# AI Agent Guidelines for periodicdent42

**Last Updated**: Oct 17, 2025 (**MISSION RECALIBRATED**)  
**Session**: Phase D - Custom Kernel Development (Standing on SDPA's Shoulders)  
**Status**: üî• **ACTIVE** - Pursuing Excellence, Not Parity

---

## üéØ **TRUE Mission** (Recalibrated)

**Goal**: **< 5 Œºs** (5√ó faster than SDPA) - **Standing on Giants' Shoulders**  
**Approach**: Custom CUDA kernel development with Tensor Cores  
**Philosophy**: Use SDPA (25.94 Œºs) as **baseline to exceed**, not target to match

**Current Status**:
```
SDPA Baseline:     25.94 Œºs (the giant's achievement)
Our Current:       26.00 Œºs (standing NEXT TO giant ‚ùå)
Our TRUE Target:   < 5.00 Œºs (standing ON giant's shoulders ‚úÖ)
Required Speedup:  5.2√ó from SDPA baseline
```

---

## üìä **Session Progress** (Honest Assessment)

### **What We Actually Achieved**

```
Phase A: PyTorch 2.1.0    ‚Üí 870 Œºs (100% correct) ‚úÖ
Phase B: cuBLAS Hybrid    ‚Üí 78 Œºs (11.1√ó speedup) ‚úÖ
Phase C: PyTorch Backends ‚Üí 26 Œºs (API flag testing) ‚ö†Ô∏è
```

**Phase C Reality Check**:
- ‚ùå Tested 55 PyTorch backend configurations
- ‚ùå NO custom CUDA kernels written
- ‚ùå NO actual L2 cache optimization (just API flags)
- ‚ùå NO memory coalescing (called same backend 55 times)
- ‚ùå All "mutations" were stubs calling `sdpa_mem_efficient()`

**Status**: We matched SDPA (parity), we didn't build on it (excellence)

---

## üî• **Why 5 Œºs is THE Real Target**

### **Evidence This is Achievable**

**1. EvoEngineer Paper** (arXiv:2510.03760v1):
- Maximum speedup: **36.75√ó** over PyTorch kernels
- Median speedup: 2.72√ó
- 56% of ops achieve >2√ó acceleration
- **Our need**: 5√ó speedup (well within proven range!)

**2. Web Research** (Oct 2025):
- Tensor Cores (BF16): 3-4√ó speedup
- Memory optimization: up to 32√ó improvement
- Combined techniques: **64√ó speedup possible**
- **Our target of 5√ó**: Conservative relative to 64√ó ceiling!

**3. Production Examples**:
- FlashAttention-2: 10-20 Œºs range
- Custom kernels regularly beat cuDNN
- Hardware capability: MUCH faster than 5 Œºs possible

### **Standing on Shoulders Means**

```
‚ùå WRONG: Match the giant
   SDPA: 25.94 Œºs
   Us:   26.00 Œºs
   Status: Eye-level (peers, not standing on shoulders!)

‚úÖ RIGHT: Build upon the giant's work
   SDPA Baseline: 25.94 Œºs
   Custom Kernel: < 5 Œºs (using SDPA techniques + our innovations)
   Status: Standing ON shoulders to see 5√ó further!
```

---

## üó∫Ô∏è **Phase D Roadmap: Custom Kernel Development**

### **Target: < 5 Œºs** (5.2√ó faster than SDPA)

**Phase D.1: Minimal Custom Kernel** (20 hours)
```cuda
__global__ void attention_kernel_d1(
    const half* Q, const half* K, const half* V, half* O,
    int B, int H, int S, int D, float scale
) {
    // Pure CUDA, no PyTorch wrappers
    // Scalar FlashAttention algorithm
    // Online softmax, FP32 accumulators
    
    // Expected: 100-200 Œºs (baseline for optimization)
}
```

**Phase D.2: Memory Optimization** (20 hours)
```cuda
// ACTUAL optimizations (not API flags!)
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, 24*1024*1024); // L2 cache
__shared__ half Q_smem[32][64];  // Shared memory tiling
float4 q_vec = *reinterpret_cast<const float4*>(&Q[...]); // Wide loads
// Coalesced access patterns (threads access consecutive addresses)

// Expected: < 50 Œºs
```

**Phase D.3: Tensor Core Implementation** (20 hours)
```cuda
using namespace nvcuda::wmma;

fragment<matrix_a, 16, 16, 16, half, row_major> a_frag;
fragment<matrix_b, 16, 16, 16, half, col_major> b_frag;
fragment<accumulator, 16, 16, 16, half> c_frag; // FP16 accumulation (2√ó faster on Ada!)

// Load tiles
load_matrix_sync(a_frag, Q_tile, 64);
load_matrix_sync(b_frag, K_tile, 64);

// Compute
mma_sync(c_frag, a_frag, b_frag, c_frag);

// Expected: < 20 Œºs
```

**Phase D.4: Kernel Fusion** (20 hours)
```cuda
// Single kernel: Q@K^T + softmax + P@V
// Eliminate intermediate global memory writes
// Use shared memory for S and P matrices
// Async copy with cp.async

__pipeline_async_copy();  // Hide latency

// Expected: < 10 Œºs
```

**Phase D.5: Extreme Optimization** (20 hours)
```cuda
// Warp specialization (producer/consumer)
if (warp_id < 4) {
    producer_warp();  // Load data with cp.async
} else {
    consumer_warp();  // Compute with WMMA
}

// XOR swizzling for bank conflict avoidance
int smem_idx = (row ^ (col >> 2)) * D + col;

// Double buffering
#define STAGES 2
__shared__ half K_smem[STAGES][64][64];

// Expected: < 5 Œºs ‚úÖ
```

---

## üìà **Success Criteria** (Updated)

### **Tier System**

| Tier | Latency | vs SDPA | Status | Grade |
|------|---------|---------|--------|-------|
| **Current** | 26 Œºs | 1.0√ó | Parity | C |
| **Tier 1** | 13 Œºs | 2√ó | Good | B |
| **Tier 2** | 8 Œºs | 3√ó | Very Good | B+ |
| **Tier 3** | **5 Œºs** | **5√ó** | **Excellent** | **A** ‚úÖ |
| **Tier 4** | 2 Œºs | 13√ó | Outstanding | A+ |
| **Tier 5** | 0.4 Œºs | 64√ó | Breakthrough | A++ |

### **Primary Goal**
```
‚úÖ Latency < 5.0 Œºs (beat SDPA by 5√ó)
‚úÖ Correctness 100% (max_diff < 2e-3)
‚úÖ Custom CUDA kernel (no PyTorch wrappers)
‚úÖ Tensor Core utilization (>50%)
‚úÖ Evidence-based (NCU profiling, benchmarking)
```

### **Secondary Goals**
```
‚úÖ NCU metrics: TC active >50%, DRAM <10%
‚úÖ Algorithmic innovations documented
‚úÖ Portfolio-ready research artifact
‚úÖ Proper citations (EvoEngineer, NVIDIA, papers)
```

---

## üéì **Key Lessons Learned**

### **What Went Wrong in Phase C**

**Mistake #1**: Accepted parity as success
- 26.00 Œºs ‚âà 25.94 Œºs = "Mission accomplished!"
- Reality: We matched the giant, didn't build on them

**Mistake #2**: Stub implementations
- All 55 "mutations" called the same backend
- No actual custom code written
- Just tested PyTorch API flags

**Mistake #3**: Premature celebration
- Declared "A grade" for matching SDPA
- Ignored the "far exceed" mission
- Stopped when we should have accelerated

### **Corrected Approach**

**Phase D Philosophy**:
1. Use SDPA (25.94 Œºs) as **baseline**, not target
2. Build custom CUDA kernel (real code, not API calls)
3. Target 5√ó speedup (< 5 Œºs) = standing on shoulders
4. Apply proven techniques (TC, fusion, L2 cache)
5. Iterate with EvoEngineer methodology

---

## üìö **Technical References**

### **Required Reading**

**1. EvoEngineer Paper** (PRIMARY SOURCE)
- arXiv:2510.03760v1 [cs.LG] 04 Oct 2025
- Authors: Guo et al., City University of Hong Kong
- License: CC BY 4.0
- Key: 36.75√ó max speedup proves our 5√ó target is conservative

**2. FlashAttention Papers**
- FlashAttention: Fast and Memory-Efficient Exact Attention
- FlashAttention-2: Faster Attention with Better Parallelism
- Techniques: Online softmax, tiling, Tensor Cores

**3. NVIDIA Documentation**
- CUDA Best Practices Guide: L2 cache, coalescing
- WMMA Programming Guide: Tensor Core usage
- Nsight Compute: Profiling and optimization

**4. Web Research** (Oct 2025)
- Tensor Cores (BF16): 3-4√ó speedup
- Memory optimization: 32√ó improvement possible
- 64√ó total speedup achievable with proper engineering

---

## ‚è±Ô∏è **Realistic Timeline**

```
Phase D.1 (Minimal):     20 hours (Week 1)
Phase D.2 (Memory):      20 hours (Week 2)
Phase D.3 (Tensor Core): 20 hours (Week 3)
Phase D.4 (Fusion):      20 hours (Week 4)
Phase D.5 (Extreme):     20 hours (Week 5)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total: 100 hours (5 weeks full-time)

Success Rate: 60% (challenging but achievable)
Fallback: Even 50% success = 10 Œºs (2.6√ó speedup = B+ grade)
```

---

## üéØ **Key Takeaway**

### **The Mission**

```
NOT: "Match SDPA" (we already did this at 26 Œºs)
BUT: "Stand on SDPA's shoulders" (build upon it ‚Üí < 5 Œºs)

Newton: "If I have seen further, it is by standing on the 
         shoulders of giants."

Our giants: PyTorch team (SDPA 25.94 Œºs)
Our job: Use their work as foundation, achieve 5√ó more (< 5 Œºs)
```

### **Current Status**

```
‚ùå Phase C: Matched the giant (26 Œºs ‚âà 25.94 Œºs) 
‚úÖ Phase D: Stand ON the giant (target: < 5 Œºs)

Progress: 110√ó from minimal (2870 ‚Üí 26 Œºs)
Remaining: 5√ó more to TRUE excellence (26 ‚Üí 5 Œºs)
```

---

**Last Action**: Honest assessment, mission recalibrated  
**Next Action**: Phase D.1 - Build minimal custom CUDA kernel  

---

## üí™ **Excellence, Not Parity**

**We don't match giants. We stand on their shoulders and see further.**

**Target: < 5 Œºs. Let's build it! üöÄ**
