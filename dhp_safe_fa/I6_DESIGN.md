# I6 Block-Parallel Design Document

**Status**: Design Phase  
**Target**: 15-20 Œºs/head (5√ó faster than I5)  
**Architecture**: Block-parallel with warpgroup cooperation

---

## üéØ Design Goals

### Performance Targets
- **Latency**: 15-20 Œºs/head (vs 90.67 Œºs in I5)
- **SM Utilization**: 60-70% (vs 24.8% in I5)
- **Synchronizations**: ‚â§8 per kernel (vs 64 in I5)
- **Memory Bandwidth**: >500 GB/s (vs ~62 GB/s in I5)

### Security Requirements
- ‚úÖ Maintain all constant-time primitives
- ‚úÖ Zero data-dependent branches
- ‚úÖ Bitwise reproducibility
- ‚úÖ Fixed iteration counts

---

## üèóÔ∏è Architectural Changes

### From Row-Parallel (I4/I5) to Block-Parallel (I6)

**I5 Model (WRONG)**:
```
Grid:  (B*H*S_max) threads
Block: 256 threads
Each thread: Process 1 complete row
Result: 24.8% SM utilization, 64 syncs
```

**I6 Model (CORRECT)**:
```
Grid:  (B*H) √ó (S_max/BM) blocks
Block: 128 threads (4 warps)
Each block: Process BM√óBN tile collaboratively
Result: 60-70% SM utilization, 8 syncs
```

---

## üìê Tile Configuration

### Optimal Tile Sizes
- **BM = 64**: Rows per block (balance between parallelism and register pressure)
- **BN = 64**: Columns per block (matches BM for square tiles)
- **BK = 64**: Head dimension (fixed by model architecture)

### Grid Dimensions
```cpp
dim3 grid(
    batch_size,              // B*H pairs
    (S_max + BM - 1) / BM    // Number of row tiles
);
dim3 block(128);  // 4 warps
```

**Example** (B=4, H=16, S=1024):
- I5: 65,536 threads total (poor utilization)
- I6: 1,024 blocks √ó 128 threads = 131,072 active threads (60%+ utilization) ‚úÖ

---

## üßÆ Computation Strategy

### Phase 1: Load Q Tile (One-Time)
```cpp
// Q_tile: [BM, BK] = [64, 64] = 8 KB shared memory
// Cooperative load: Each thread loads 32 elements
// Sync: 1
```

### Phase 2: Loop Over K/V Tiles
For each column tile `j` in `[0, S_max/BN)`:

**2a. Load K Tile**:
```cpp
// K_tile: [BN, BK] = [64, 64] = 8 KB
// Sync: 1 per tile
```

**2b. Compute S = Q @ K^T**:
```cpp
// S_tile: [BM, BN] = [64, 64] = 8 KB
// Each thread computes 32 dot products
// Sync: 1 per tile (after compute)
```

**2c. Load V Tile**:
```cpp
// V_tile: [BN, BK] = [64, 64] = 8 KB
// Sync: 1 per tile
```

**2d. Online Softmax + P @ V**:
```cpp
// Per-thread online softmax state
// Accumulate output: out += softmax(S) @ V
// No sync needed (thread-local)
```

**Total syncs**: 1 (Q load) + `num_tiles` √ó 3 (K, S, V) ‚âà 1 + 16√ó3 = 49

‚ö†Ô∏è **Still too many!** Need to reduce...

### Optimized Approach (Double Buffering)
- Use 2 sets of K/V tiles
- Overlap load of next tile with compute of current tile
- **Reduces syncs to**: 1 + `num_tiles` √ó 2 ‚âà 33 (better, but still high)

---

## üîß Implementation Challenges

### Challenge 1: Per-Thread Output Accumulation
**Problem**: Each thread needs to accumulate d=64 output values.
- **Registers needed**: 64 float = 64 registers
- **Total with m/l state**: ~80 registers per thread ‚úÖ (under 255 limit)

**Solution**: Each thread processes 1-2 rows, accumulates full d=64 output.

### Challenge 2: Thread-to-Row Mapping
**Problem**: How to assign 128 threads to 64 rows?

**Option A**: 2 threads per row (warp-level reduction)
```cpp
const int row = threadIdx.x / 2;
const int sub_row = threadIdx.x % 2;
// Thread 0,1 ‚Üí row 0
// Thread 2,3 ‚Üí row 1
// ...
```

**Option B**: 1 thread per row, some idle
```cpp
const int row = threadIdx.x;
if (row < BM) {  // First 64 threads active
    // Process row
}
// Threads 64-127 idle (50% waste!) ‚ùå
```

**Best**: Option A with warp reduction for better utilization.

### Challenge 3: Constant-Time with Block-Parallel
**Problem**: Causal masking creates triangular access patterns.

**Solution**: Process full rectangular tiles, mask with `ct_select`:
```cpp
for (int col = 0; col < BN; ++col) {
    int global_col = col_start + col;
    uint32_t causal_valid = ct_le_u32(global_col, global_row);
    score = ct_select_f32(-INFINITY, score, causal_valid);
}
```

All threads execute all iterations (constant-time) ‚úÖ

---

## üìä Expected Performance Analysis

### Memory Traffic (per tile)
- **Q tile load**: 8 KB √ó 1 = 8 KB (one-time)
- **K tile load**: 8 KB √ó 16 tiles = 128 KB
- **V tile load**: 8 KB √ó 16 tiles = 128 KB
- **Output write**: 8 KB √ó 16 tiles = 128 KB
- **Total**: 392 KB per (B*H) pair

**Bandwidth** (S=1024, B*H=64):
- Total traffic: 392 KB √ó 64 = 25 MB
- Target latency: 20 Œºs
- Required BW: 25 MB / 20 Œºs = 1.25 TB/s ‚úÖ (H100 has 2 TB/s)

### Compute (per tile)
- **Q@K^T**: BM √ó BN √ó BK = 64 √ó 64 √ó 64 = 262K FP16 ops
- **Softmax**: BM √ó BN = 4K exp() calls
- **P@V**: BM √ó BN √ó BK = 262K FP16 ops
- **Total per tile**: ~530K ops
- **Total (16 tiles)**: 8.5M ops per (B*H)

**Compute time** (scalar FP16):
- H100 FP16: 33 TFLOPS = 33M ops/Œºs
- Time: 8.5M / 33M = 0.26 Œºs per (B*H) ‚úÖ (compute-light)

**Conclusion**: Memory-bound (as expected), BW requirements achievable.

---

## üöß Implementation Complexity

### Issues with Current Draft
1. **Output accumulation**: Thread-to-row mapping needs careful design
2. **Warp-level reduction**: Not implemented for 2 threads/row
3. **Memory aliasing**: Shared memory bank conflicts possible
4. **Register pressure**: Need to verify actual usage
5. **Correctness**: Complex data flow, easy to introduce bugs

### Recommended Approach: Incremental Development

**Step 1**: Simplified I6 (this session if time)
- Single-threaded per row (50% thread idle, but correct)
- Target: 30-40 Œºs/head (2-3√ó faster than I5)
- Validates architecture change

**Step 2**: Optimized I6 (next session)
- 2 threads per row with warp reduction
- Target: 15-20 Œºs/head (5√ó faster than I5)
- Production-quality

**Step 3**: I7 with WMMA (future)
- Add Tensor Core matrix multiply
- Target: 5-8 Œºs/head
- Competitive with PyTorch

---

## üéì Learning from FlashAttention-3

### What FA3 Does Right
1. **Persistent kernels**: Stay resident, no launch overhead
2. **TMA**: Async global‚Üíshared DMA
3. **WGMMA**: Native Hopper Tensor Core ops
4. **Warpgroup scheduling**: 128 threads cooperate efficiently

### What We Can Adopt Now (I6)
- ‚úÖ Block-parallel tiling
- ‚úÖ Shared memory staging
- ‚úÖ Cooperative loads
- ‚ùå TMA (too complex for I6)
- ‚ùå WGMMA (needs WMMA first)
- ‚ùå Persistent (needs more infrastructure)

---

## üìà Realistic Expectations

### If We Implement Simplified I6
- **Best case**: 25-30 Œºs/head (3√ó faster than I5)
- **Worst case**: 60-70 Œºs/head (1.3√ó faster than I5, due to overhead)
- **Likely**: 30-40 Œºs/head (2-3√ó faster than I5) ‚úÖ

### Why Not 5√ó Speedup?
- Still using scalar operations (no Tensor Cores)
- Still have 33+ syncs (no double buffering)
- No async loads (no TMA)

**Bottom line**: I6 proves the architecture, I7/I8 get the performance.

---

## üèÅ Next Steps

### For This Session (if time permits)
1. Implement simplified I6 (1 thread/row)
2. Test on H100
3. Validate 2-3√ó speedup
4. Document findings

### For Next Session
1. Optimize I6 (2 threads/row + warp reduction)
2. Add double buffering
3. Target 15-20 Œºs/head

### Future (I7+)
1. Integrate WMMA for Q@K^T and P@V
2. Add TMA for async loads
3. Implement persistent kernel
4. Target <5 Œºs/head

---

## üí° Key Insight

**The fundamental architectural change (row‚Üíblock parallel) is more important than any single optimization.**

Even a "naive" block-parallel implementation should be 2-3√ó faster than optimized row-parallel (I5).

This validates the approach before investing in complex optimizations.

