name: Performance CI

on:
  pull_request:
    paths:
      - 'csrc/**'
      - 'bench/**'
      - 'cudadent42/**'
      - '.github/workflows/perf_ci.yml'
  workflow_dispatch:
    inputs:
      run_profiling:
        description: 'Run Nsight Compute profiling'
        required: false
        default: 'false'

jobs:
  perf-check:
    name: Performance Validation
    runs-on: ubuntu-latest
    # TODO: Replace with self-hosted GPU runner
    # runs-on: [self-hosted, gpu, l4]
    
    steps:
      - name: Check if performance validation needed
        id: should_run
        run: |
          # Skip for evidence/documentation PRs
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # Check if PR has evidence or documentation labels
            if [[ "${{ contains(github.event.pull_request.labels.*.name, 'evidence') }}" == "true" ]] || \
               [[ "${{ contains(github.event.pull_request.labels.*.name, 'documentation') }}" == "true" ]]; then
              echo "skip=true" >> $GITHUB_OUTPUT
              echo "âš ï¸  Skipping performance validation for evidence/documentation PR"
              echo "::notice::Performance validation skipped - evidence-focused PR"
            else
              echo "skip=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Checkout code
        if: steps.should_run.outputs.skip != 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for git operations
      
      - name: Set up Python
        if: steps.should_run.outputs.skip != 'true'
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Cache pip dependencies
        if: steps.should_run.outputs.skip != 'true'
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        if: steps.should_run.outputs.skip != 'true'
        continue-on-error: true
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt || echo "âš ï¸  Some dependencies failed to install"
          fi
          pip install ninja ccache pytest pytest-cov scipy || echo "âš ï¸  Some test dependencies unavailable"
      
      - name: Setup CUDA environment (if GPU available)
        if: steps.should_run.outputs.skip != 'true' && runner.os == 'Linux'
        continue-on-error: true
        run: |
          if command -v nvidia-smi &> /dev/null; then
            echo "GPU detected:"
            nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv
            
            # Setup build environment
            if [ -f scripts/setup_dev_env.sh ]; then
              bash scripts/setup_dev_env.sh
              source ~/.bashrc
            fi
          else
            echo "âš ï¸  No GPU detected - will skip GPU-dependent tests"
          fi
      
      - name: Run correctness fuzzing
        if: steps.should_run.outputs.skip != 'true'
        id: correctness
        continue-on-error: true
        run: |
          if [ ! -f cudadent42/bench/correctness_fuzz.py ]; then
            echo "âš ï¸  correctness_fuzz.py not found - skipping"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Running correctness fuzzing..."
          python3 cudadent42/bench/correctness_fuzz.py || exit_code=$?
          
          if [ $exit_code -eq 2 ]; then
            echo "âš ï¸  Correctness fuzzing skipped (no custom kernel)"
            echo "status=skipped" >> $GITHUB_OUTPUT
          elif [ $exit_code -eq 0 ]; then
            echo "âœ… Correctness fuzzing passed"
            echo "status=passed" >> $GITHUB_OUTPUT
          else
            echo "âŒ Correctness fuzzing failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Run baseline benchmark (S=512)
        id: baseline
        if: steps.should_run.outputs.skip != 'true' && steps.correctness.outputs.status != 'failed'
        continue-on-error: true
        run: |
          if [ ! -f cudadent42/bench/baseline_comprehensive.py ]; then
            echo "âš ï¸  baseline_comprehensive.py not found - skipping"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Running baseline benchmark..."
          python3 cudadent42/bench/baseline_comprehensive.py --only s512
          
          if [ -f cudadent42/bench/artifacts/baseline_comprehensive/summary.json ]; then
            echo "âœ… Baseline benchmark complete"
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "âŒ Baseline benchmark failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Compare to baseline
        id: comparison
        if: steps.baseline.outputs.status == 'success'
        continue-on-error: true
        run: |
          echo "Comparing candidate to baseline..."
          
          python3 cudadent42/bench/ci_compare.py \
            --baseline .ci/baseline_s512.json \
            --candidate cudadent42/bench/artifacts/baseline_comprehensive/summary.json \
            --json > comparison.json
          
          exit_code=$?
          
          # Parse results
          cat comparison.json
          
          verdict=$(jq -r '.verdict' comparison.json)
          delta=$(jq -r '.delta_pct' comparison.json)
          
          echo "verdict=$verdict" >> $GITHUB_OUTPUT
          echo "delta=$delta" >> $GITHUB_OUTPUT
          
          if [ $exit_code -eq 1 ]; then
            echo "âŒ Performance regression detected"
            echo "status=regression" >> $GITHUB_OUTPUT
            exit 1
          elif [ $exit_code -eq 0 ]; then
            echo "âœ… Performance maintained or improved"
            echo "status=pass" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸  No significant difference"
            echo "status=inconclusive" >> $GITHUB_OUTPUT
          fi
      
      - name: Run Nsight Compute profiling
        id: profiling
        if: |
          github.event.inputs.run_profiling == 'true' || 
          steps.comparison.outputs.status == 'pass'
        continue-on-error: true
        run: |
          if ! command -v ncu &> /dev/null; then
            echo "âš ï¸  Nsight Compute not available - skipping profiling"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          echo "Running Nsight Compute profiling..."
          S=512 B=32 H=8 D=64 bash scripts/profile_sdpa.sh
          
          if [ -f artifacts/ncu/sdpa_s512_b32_h8_d64.ncu-rep ]; then
            echo "âœ… Profiling complete"
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "âš ï¸  Profiling failed"
            echo "status=failed" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-ci-artifacts
          path: |
            cudadent42/bench/artifacts/
            artifacts/ncu/
            comparison.json
          retention-days: 30
      
      - name: Comment on PR
        if: github.event_name == 'pull_request' && steps.comparison.outputs.status != ''
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## âš¡ Performance CI Results\n\n';
            
            // Correctness
            const correctness = '${{ steps.correctness.outputs.status }}';
            if (correctness === 'passed') {
              comment += 'âœ… **Correctness Fuzzing**: All 27 tests passed\n';
            } else if (correctness === 'skipped') {
              comment += 'âš ï¸  **Correctness Fuzzing**: Skipped (no custom kernel)\n';
            } else if (correctness === 'failed') {
              comment += 'âŒ **Correctness Fuzzing**: FAILED\n';
            }
            
            // Performance comparison
            const status = '${{ steps.comparison.outputs.status }}';
            const verdict = '${{ steps.comparison.outputs.verdict }}';
            const delta = '${{ steps.comparison.outputs.delta }}';
            
            comment += '\n### Performance Comparison\n\n';
            
            if (status === 'regression') {
              comment += `âŒ **Regression Detected**: ${delta}% slower\n\n`;
              comment += `**Verdict**: ${verdict}\n\n`;
              comment += '**Action Required**: This PR introduces a performance regression. ';
              comment += 'Please investigate and fix before merging.\n';
            } else if (status === 'pass') {
              comment += `âœ… **Performance Maintained**: ${delta}% change\n\n`;
              comment += `**Verdict**: ${verdict}\n`;
            } else if (status === 'inconclusive') {
              comment += `âš ï¸  **No Significant Difference**: ${delta}% change\n\n`;
              comment += `**Verdict**: ${verdict}\n`;
            }
            
            // Profiling
            const profiling = '${{ steps.profiling.outputs.status }}';
            if (profiling === 'success') {
              comment += '\nâœ… **Nsight Profiling**: Complete (see artifacts)\n';
            } else if (profiling === 'skipped') {
              comment += '\nâš ï¸  **Nsight Profiling**: Skipped\n';
            }
            
            // Artifacts
            comment += '\n### ðŸ“Š Artifacts\n\n';
            comment += '- Baseline results: `cudadent42/bench/artifacts/baseline_comprehensive/`\n';
            comment += '- Comparison: `comparison.json`\n';
            if (profiling === 'success') {
              comment += '- Nsight reports: `artifacts/ncu/*.ncu-rep`\n';
            }
            
            // Documentation
            comment += '\n### ðŸ“š Documentation\n\n';
            comment += '- [Performance Guardrails](docs/perf_guardrails.md)\n';
            comment += '- [Dev Environment Setup](docs/dev_env.md)\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Fail if regression
        if: steps.should_run.outputs.skip != 'true' && steps.comparison.outputs.status == 'regression'
        run: |
          echo "âŒ CI failed due to performance regression"
          exit 1
      
      - name: Summary
        if: always()
        run: |
          echo "## Performance CI Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.should_run.outputs.skip }}" == "true" ]; then
            echo "âš ï¸  **Performance validation skipped**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "This PR is marked as evidence/documentation." >> $GITHUB_STEP_SUMMARY
            echo "Performance validation will run for performance-focused changes." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Evidence-Based Validation" >> $GITHUB_STEP_SUMMARY
            echo "This PR provides code-level evidence:" >> $GITHUB_STEP_SUMMARY
            echo "- See PR description for artifacts" >> $GITHUB_STEP_SUMMARY
            echo "- SASS proof and sanitizer status available" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Correctness**: ${{ steps.correctness.outputs.status }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Baseline**: ${{ steps.baseline.outputs.status }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Comparison**: ${{ steps.comparison.outputs.status }}" >> $GITHUB_STEP_SUMMARY
            echo "- **Profiling**: ${{ steps.profiling.outputs.status }}" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "${{ steps.comparison.outputs.verdict }}" != "" ]; then
              echo "**Verdict**: ${{ steps.comparison.outputs.verdict }}" >> $GITHUB_STEP_SUMMARY
            fi
          fi

