#!/usr/bin/env python3
"""
Conformal Expected Improvement (Conformal-EI)

Novel contribution: EI acquisition weighted by conformal prediction credibility.
Reduces mis-acquisitions while maintaining discovery rate.

Citation alignment:
- Split conformal prediction (Vovk et al., 2005; Shafer & Vovk, 2008)
- Conformal for scientific ML (Stanton et al., 2022; Cognac et al., 2023)
- Active learning + calibration (Cocheteux et al., 2025; Widmann et al., 2021)

Author: GOATnote Autonomous Research Lab Initiative
Contact: b@thegoatnote.com
License: MIT
"""

import numpy as np
import torch
import pandas as pd
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from scipy import stats
import json
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import logging

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from phase10_gp_active_learning.data.uci_loader import load_uci_superconductor
from phase10_gp_active_learning.models.dkl_model import create_dkl_model
from phase10_gp_active_learning.models.botorch_dkl import BoTorchDKL
from botorch.acquisition import ExpectedImprovement

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

sns.set_style('whitegrid')
plt.rcParams['figure.dpi'] = 300


class ConformalPredictor:
    """
    Split conformal prediction for regression.
    
    Provides distribution-free prediction intervals with guaranteed coverage.
    """
    
    def __init__(self, alpha=0.1):
        """
        Args:
            alpha: Miscoverage rate (0.1 = 90% coverage)
        """
        self.alpha = alpha
        self.quantile = None
        
    def calibrate(self, predictions: np.ndarray, targets: np.ndarray):
        """
        Calibrate on validation set.
        
        Args:
            predictions: Model predictions on calibration set
            targets: True values
        """
        # Compute nonconformity scores (absolute errors)
        scores = np.abs(predictions - targets)
        
        # Quantile at 1-alpha level
        n = len(scores)
        q_level = np.ceil((n + 1) * (1 - self.alpha)) / n
        self.quantile = np.quantile(scores, q_level)
        
        logger.info(f"Conformal calibration: Î±={self.alpha}, quantile={self.quantile:.3f}")
        
    def predict(self, predictions: np.ndarray, return_credibility=False):
        """
        Generate conformal prediction intervals.
        
        Args:
            predictions: Model point predictions
            return_credibility: If True, return credibility scores (1 - nonconformity)
            
        Returns:
            lower, upper: Prediction interval bounds
            OR credibility: Conformity scores if return_credibility=True
        """
        if self.quantile is None:
            raise ValueError("Must calibrate first")
        
        if return_credibility:
            # Credibility = how confident we are (inverse of interval width)
            # Higher credibility = narrower interval = more confident
            credibility = 1.0 / (1.0 + self.quantile)
            return np.full(len(predictions), credibility)
        else:
            lower = predictions - self.quantile
            upper = predictions + self.quantile
            return lower, upper
        
    def coverage(self, predictions: np.ndarray, targets: np.ndarray):
        """Compute empirical coverage on test set"""
        lower, upper = self.predict(predictions)
        covered = (targets >= lower) & (targets <= upper)
        return covered.mean()


def conformal_ei_acquisition(
    model,
    X_candidates: torch.Tensor,
    best_f: float,
    conformal_predictor: ConformalPredictor,
    credibility_weight: float = 0.5
):
    """
    Conformal Expected Improvement acquisition function.
    
    EI_conformal(x) = EI(x) * (1 + w * credibility(x))
    
    where credibility comes from conformal prediction intervals.
    
    Args:
        model: BoTorch-compatible model
        X_candidates: Candidate points (N, 1, D)
        best_f: Current best observation
        conformal_predictor: Calibrated conformal predictor
        credibility_weight: Weight for credibility term (0 = vanilla EI, 1 = full conformal)
        
    Returns:
        acquisition_values: Conformal-EI scores
    """
    # Standard EI
    acq_ei = ExpectedImprovement(model=model, best_f=best_f)
    
    with torch.no_grad():
        ei_values = acq_ei(X_candidates).cpu().numpy()
        
        # Get predictions for credibility
        posterior = model.posterior(X_candidates)
        predictions = posterior.mean.cpu().numpy().ravel()
        
    # Conformal credibility
    credibility = conformal_predictor.predict(predictions, return_credibility=True)
    
    # Weighted combination
    conformal_ei = ei_values * (1.0 + credibility_weight * credibility)
    
    return torch.tensor(conformal_ei, dtype=torch.float64)


def run_active_learning(
    X_pool: np.ndarray,
    y_pool: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    method: str = "conformal_ei",  # "conformal_ei" or "vanilla_ei"
    initial_samples: int = 100,
    num_rounds: int = 20,
    batch_size: int = 1,
    alpha: float = 0.1,  # Conformal miscoverage rate
    credibility_weight: float = 0.5,
    random_seed: int = 42
):
    """
    Run active learning with Conformal-EI or vanilla EI.
    
    Returns:
        metrics: Dict with RMSE history, coverage history, regret history
    """
    np.random.seed(random_seed)
    torch.manual_seed(random_seed)
    
    logger.info(f"ðŸš€ Running {method} (seed={random_seed})")
    
    # Split pool into train/calibration
    n_pool = len(X_pool)
    indices = np.random.permutation(n_pool)
    
    # 80% train pool, 20% calibration
    n_calib = int(0.2 * n_pool)
    calib_indices = indices[:n_calib]
    train_indices = indices[n_calib:]
    
    X_calib = X_pool[calib_indices]
    y_calib = y_pool[calib_indices]
    X_train_pool = X_pool[train_indices]
    y_train_pool = y_pool[train_indices]
    
    # Initialize labeled set from train pool
    initial_idx = np.random.choice(len(X_train_pool), initial_samples, replace=False)
    X_labeled = X_train_pool[initial_idx]
    y_labeled = y_train_pool[initial_idx]
    
    remaining_idx = np.setdiff1d(np.arange(len(X_train_pool)), initial_idx)
    X_unlabeled = X_train_pool[remaining_idx]
    y_unlabeled = y_train_pool[remaining_idx]
    
    # Scalers
    X_scaler = StandardScaler()
    y_scaler = StandardScaler()
    
    # Metrics
    rmse_history = []
    coverage_history = []
    regret_history = []  # How often we pick sub-optimal points
    
    for round_idx in range(num_rounds):
        # Scale data
        X_labeled_scaled = X_scaler.fit_transform(X_labeled)
        y_labeled_scaled = y_scaler.fit_transform(y_labeled.reshape(-1, 1)).ravel()
        X_test_scaled = X_scaler.transform(X_test)
        X_calib_scaled = X_scaler.transform(X_calib)
        X_unlabeled_scaled = X_scaler.transform(X_unlabeled)
        
        # Train DKL
        dkl = create_dkl_model(
            X_labeled_scaled, y_labeled_scaled,
            input_dim=X_labeled_scaled.shape[1],
            n_epochs=20,
            verbose=False
        )
        model = BoTorchDKL(dkl)
        
        # Evaluate on test set
        X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float64)
        with torch.no_grad():
            posterior = model.posterior(X_test_tensor)
            y_pred_scaled = posterior.mean.cpu().numpy().ravel()
        
        y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        rmse_history.append(rmse)
        
        # Conformal calibration
        if method == "conformal_ei":
            with torch.no_grad():
                X_calib_tensor = torch.tensor(X_calib_scaled, dtype=torch.float64)
                posterior_calib = model.posterior(X_calib_tensor)
                y_calib_pred_scaled = posterior_calib.mean.cpu().numpy().ravel()
            
            y_calib_pred = y_scaler.inverse_transform(y_calib_pred_scaled.reshape(-1, 1)).ravel()
            
            conformal = ConformalPredictor(alpha=alpha)
            conformal.calibrate(y_calib_pred, y_calib)
            
            # Measure coverage on test set
            y_test_pred_scaled = y_pred_scaled
            y_test_pred = y_pred
            coverage = conformal.coverage(y_test_pred, y_test)
            coverage_history.append(coverage)
        else:
            coverage_history.append(np.nan)
        
        logger.info(f"   Round {round_idx:2d}: n_labeled={len(X_labeled):4d}, RMSE={rmse:.2f} K, "
                   f"Coverage={coverage_history[-1]:.3f}")
        
        # Acquisition
        if round_idx < num_rounds - 1 and len(X_unlabeled) > 0:
            best_f_scaled = y_labeled_scaled.max()
            X_unlabeled_tensor = torch.tensor(X_unlabeled_scaled, dtype=torch.float64)
            
            if X_unlabeled_tensor.dim() == 2:
                X_unlabeled_tensor = X_unlabeled_tensor.unsqueeze(1)
            
            if method == "conformal_ei":
                acq_values = conformal_ei_acquisition(
                    model, X_unlabeled_tensor, best_f_scaled,
                    conformal, credibility_weight
                )
            else:  # vanilla_ei
                acq = ExpectedImprovement(model=model, best_f=best_f_scaled)
                acq_values = acq(X_unlabeled_tensor).squeeze()
            
            best_idx = acq_values.argmax().item()
            
            # Compute regret (how far from optimal in unlabeled pool)
            best_available = y_unlabeled.max()
            selected_value = y_unlabeled[best_idx]
            regret = best_available - selected_value
            regret_history.append(regret)
            
            # Update sets
            X_labeled = np.vstack([X_labeled, X_unlabeled[best_idx:best_idx+1]])
            y_labeled = np.append(y_labeled, y_unlabeled[best_idx])
            X_unlabeled = np.delete(X_unlabeled, best_idx, axis=0)
            y_unlabeled = np.delete(y_unlabeled, best_idx)
    
    return {
        'rmse_history': rmse_history,
        'coverage_history': coverage_history,
        'regret_history': regret_history,
        'final_rmse': rmse_history[-1],
        'mean_coverage': np.nanmean(coverage_history),
        'mean_regret': np.mean(regret_history)
    }


def main():
    logger.info("="*70)
    logger.info("CONFORMAL-EI NOVELTY EXPERIMENT")
    logger.info("="*70)
    
    # Load data
    logger.info("\nðŸ“‚ Loading UCI dataset...")
    train_df, val_df, test_df = load_uci_superconductor()
    
    feature_cols = [col for col in train_df.columns if col != 'Tc']
    
    # Pool = train + val
    X_pool = pd.concat([train_df[feature_cols], val_df[feature_cols]]).values
    y_pool = pd.concat([train_df['Tc'], val_df['Tc']]).values
    X_test = test_df[feature_cols].values
    y_test = test_df['Tc'].values
    
    logger.info(f"âœ… Pool: {len(X_pool)}, Test: {len(X_test)}")
    
    # Run experiments
    seeds = list(range(42, 42 + 5))  # 5 seeds for now (can scale to 20)
    
    results = {
        'conformal_ei': [],
        'vanilla_ei': []
    }
    
    for method in ['conformal_ei', 'vanilla_ei']:
        logger.info(f"\n{'='*70}")
        logger.info(f"METHOD: {method.upper()}")
        logger.info(f"{'='*70}")
        
        for seed in seeds:
            metrics = run_active_learning(
                X_pool.copy(), y_pool.copy(),
                X_test.copy(), y_test.copy(),
                method=method,
                initial_samples=100,
                num_rounds=20,
                alpha=0.1,  # 90% coverage
                credibility_weight=0.5,
                random_seed=seed
            )
            results[method].append(metrics)
    
    # Aggregate statistics
    logger.info(f"\n{'='*70}")
    logger.info("RESULTS")
    logger.info(f"{'='*70}")
    
    for method in ['conformal_ei', 'vanilla_ei']:
        final_rmses = [r['final_rmse'] for r in results[method]]
        mean_coverages = [r['mean_coverage'] for r in results[method]]
        mean_regrets = [r['mean_regret'] for r in results[method]]
        
        logger.info(f"\n{method.upper()}:")
        logger.info(f"  RMSE: {np.mean(final_rmses):.2f} Â± {np.std(final_rmses):.2f} K")
        logger.info(f"  Coverage: {np.nanmean(mean_coverages):.3f} Â± {np.nanstd(mean_coverages):.3f}")
        logger.info(f"  Mean Regret: {np.mean(mean_regrets):.2f} Â± {np.std(mean_regrets):.2f} K")
    
    # Paired statistics
    conformal_rmses = [r['final_rmse'] for r in results['conformal_ei']]
    vanilla_rmses = [r['final_rmse'] for r in results['vanilla_ei']]
    
    t_stat, p_value = stats.ttest_rel(conformal_rmses, vanilla_rmses)
    
    logger.info(f"\n{'='*70}")
    logger.info("PAIRED COMPARISON")
    logger.info(f"{'='*70}")
    logger.info(f"Conformal-EI vs Vanilla-EI: p={p_value:.4f}")
    
    if p_value < 0.05:
        improvement = (np.mean(vanilla_rmses) - np.mean(conformal_rmses)) / np.mean(vanilla_rmses) * 100
        logger.info(f"âœ… SIGNIFICANT: {improvement:.1f}% improvement (p<0.05)")
    else:
        logger.info(f"âŒ NOT SIGNIFICANT (pâ‰¥0.05)")
    
    # Save results
    output_dir = Path("experiments/novelty")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_dir / "conformal_ei_results.json", 'w') as f:
        json.dump({
            'experiment': 'Conformal-EI Novelty',
            'n_seeds': len(seeds),
            'results': {
                'conformal_ei': {
                    'mean_rmse': float(np.mean(conformal_rmses)),
                    'std_rmse': float(np.std(conformal_rmses)),
                    'mean_coverage': float(np.nanmean([r['mean_coverage'] for r in results['conformal_ei']])),
                    'mean_regret': float(np.mean([r['mean_regret'] for r in results['conformal_ei']]))
                },
                'vanilla_ei': {
                    'mean_rmse': float(np.mean(vanilla_rmses)),
                    'std_rmse': float(np.std(vanilla_rmses)),
                    'mean_regret': float(np.mean([r['mean_regret'] for r in results['vanilla_ei']]))
                }
            },
            'comparison': {
                'p_value': float(p_value),
                'significant': bool(p_value < 0.05)
            }
        }, f, indent=2)
    
    logger.info(f"\nâœ… Results saved to: {output_dir / 'conformal_ei_results.json'}")
    logger.info("="*70)
    logger.info("âœ… CONFORMAL-EI EXPERIMENT COMPLETE")
    logger.info("="*70)


if __name__ == '__main__':
    main()

